{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 960\n",
    "hist_size = 30\n",
    "data_dict = {}\n",
    "feed_dict = {}\n",
    "batch_idx = 0\n",
    "feature_size = 1048573\n",
    "epoch = 25\n",
    "\n",
    "def data_set(data_dict, feature, string):\n",
    "        if string not in data_dict:\n",
    "             data_dict[string] =[[feature]]\n",
    "        else:\n",
    "             if(len(data_dict[string]) < batch_idx + 1):\n",
    "                 data_dict[string].append([feature])\n",
    "             else:\n",
    "                 data_dict[string][batch_idx].append(feature)\n",
    "\n",
    "def input_data_set(data_dict, features, prefix=\"\"):\n",
    "    for feature in features:\n",
    "        feature = feature.split(\":\")\n",
    "        feature = int(feature[0])\n",
    "        group_id = feature >> 48\n",
    "        feature = feature % feature_size \n",
    "        data_set(data_dict, feature, prefix+str(group_id))\n",
    "\n",
    "def input_hist_data_set(data_dict, hist_features, hist_group_ids, pos_group_ids, hist_size, prefix=\"\"):\n",
    "    hist_len = len(hist_features)\n",
    "    if hist_features[0] == '\\n' or hist_features[0] == '' or hist_features[0] == ' ':\n",
    "          hist_len = 0\n",
    "    for i in range(0, hist_size):\n",
    "        if i < hist_len:\n",
    "            features = hist_features[i].split()\n",
    "            for feature in features:\n",
    "                 feature = feature.split(\":\")\n",
    "                 feature = int(feature[0])\n",
    "                 group_id = feature >> 48\n",
    "                 feature = feature % feature_size\n",
    "                 if group_id in pos_group_ids:\n",
    "                       data_set(data_dict, feature, prefix+\"position_\"+str(i)+\"_\"+str(group_id))\n",
    "                 else:\n",
    "                       data_set(data_dict, feature, prefix+str(i)+\"_\"+str(group_id))\n",
    "        else:\n",
    "            for group_id in hist_group_ids:\n",
    "                 data_set(data_dict, 0, prefix+str(i)+\"_\"+str(group_id))\n",
    "            for group_id in pos_group_ids:\n",
    "                 data_set(data_dict, 0, prefix+\"position_\"+str(i)+\"_\"+str(group_id))\n",
    "             \n",
    "    if prefix+\"histLen\" not in data_dict:\n",
    "            data_dict[prefix+\"histLen\"] = [hist_len]\n",
    "    else:\n",
    "            data_dict[prefix+\"histLen\"].append(hist_len)\n",
    "\n",
    "def data_dict_sparse_feature(data_dict, string):\n",
    "    index, value = [], []\n",
    "    for i in range(batch_size):\n",
    "           for k in range(len(data_dict[string][i])):\n",
    "                index.append(np.array([i, k], dtype = np.int64))\n",
    "                value.append(data_dict[string][i][k])\n",
    "    iv = tf.sparse.SparseTensor(index, value, [len(data_dict[string]), feature_size])\n",
    "    data_dict[string] = iv\n",
    "\n",
    "\n",
    "def train_data_process(data, main_group_ids, candidate_group_ids, clicked_group_ids, unclick_group_ids, feedback_group_ids, pos_group_ids):\n",
    "    global data_dict, feed_dict, batch_idx, batch_size\n",
    "    data = data.split('\\t')\n",
    "    label = float(data[0])\n",
    "    weight = float(data[1])\n",
    "    features = data[2].split('|')\n",
    "    main_features = features[0].split()\n",
    "    candidate_features = features[1].split()\n",
    "    clicked_features = features[2].split(';')\n",
    "    unclick_features = features[3].split(';')\n",
    "    feedback_features = features[4].split(';')\n",
    "    if \"label\" not in data_dict:\n",
    "        data_dict[\"label\"] = [label]\n",
    "    else:\n",
    "        data_dict[\"label\"].append(label)\n",
    "    \n",
    "    if \"weight\" not in data_dict:\n",
    "        data_dict[\"weight\"] = [weight]\n",
    "    else:\n",
    "        data_dict[\"weight\"].append(weight)\n",
    "    \n",
    "    input_data_set(data_dict, main_features, \"main_\")\n",
    "    input_data_set(data_dict, candidate_features, \"candidate_\")\n",
    "    input_hist_data_set(data_dict, clicked_features, clicked_group_ids, pos_group_ids, hist_size, \"clicked_\")\n",
    "    input_hist_data_set(data_dict, unclick_features, unclick_group_ids, pos_group_ids, hist_size, \"unclick_\")\n",
    "    input_hist_data_set(data_dict, feedback_features, feedback_group_ids, pos_group_ids, hist_size, \"feedback_\")\n",
    "\n",
    "\n",
    "def data_gen(path):\n",
    "    global batch_idx, batch_size, main_group_ids, candidate_group_ids, clicked_group_ids, unclick_group_ids, feedback_group_ids, pos_group_ids\n",
    "    while True:\n",
    "        f = path.open(mode='r')\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            train_data_process(line, main_group_ids, candidate_group_ids, clicked_group_ids, unclick_group_ids, feedback_group_ids, pos_group_ids)\n",
    "            if batch_idx < batch_size -1: \n",
    "                batch_idx += 1\n",
    "            else:\n",
    "                for group_id in main_group_ids:\n",
    "                    data_name = \"main_\" + str(group_id)\n",
    "                    data_dict_sparse_feature(data_dict, data_name)\n",
    "                for group_id in candidate_group_ids:\n",
    "                    data_name = \"candidate_\" + str(group_id)\n",
    "                    data_dict_sparse_feature(data_dict, data_name)\n",
    "                for i in range(hist_size):\n",
    "                    for group_id in clicked_group_ids:\n",
    "                        data_name = \"clicked_\" + str(i) + \"_\" + str(group_id)\n",
    "                        data_dict_sparse_feature(data_dict, data_name) \n",
    "                    for group_id in unclick_group_ids:\n",
    "                        data_name = \"unclick_\" + str(i) + \"_\" + str(group_id)\n",
    "                        data_dict_sparse_feature(data_dict, data_name) \n",
    "                    for group_id in feedback_group_ids:\n",
    "                        data_name = \"feedback_\" + str(i) + \"_\" + str(group_id)\n",
    "                        data_dict_sparse_feature(data_dict, data_name)\n",
    "                    for group_id in pos_group_ids:   \n",
    "                        data_name = \"clicked_position_\" + str(i) + \"_\" + str(group_id)\n",
    "                        data_dict_sparse_feature(data_dict, data_name)\n",
    "                        data_name = \"unclick_position_\" + str(i) + \"_\" + str(group_id)\n",
    "                        data_dict_sparse_feature(data_dict, data_name)\n",
    "                        data_name = \"feedback_position_\" + str(i) + \"_\" + str(group_id)\n",
    "                        data_dict_sparse_feature(data_dict, data_name)\n",
    "                data_input = {k: v for k, v in data_dict.items() if k != \"label\" and k != \"weight\"}\n",
    "                yield (data_input, data_dict[\"label\"], data_dict[\"weight\"])\n",
    "            line = f.readline()\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\Volumes\\\\D\\\\guohao\\\\resys\\\\dfn\\\\example'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-bb59cd4ff72d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/Volumes/D/guohao/resys/dfn/example\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-6f7f30704abe>\u001b[0m in \u001b[0;36mdata_gen\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_group_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_group_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclicked_group_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munclick_group_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeedback_group_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_group_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m         return io.open(str(self), mode, buffering, encoding, errors, newline,\n\u001b[1;32m-> 1181\u001b[1;33m                        opener=self._opener)\n\u001b[0m\u001b[0;32m   1182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\pathlib.py\u001b[0m in \u001b[0;36m_opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1033\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0o666\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m         \u001b[1;31m# A stub for the opener argument to built-in open()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1035\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_raw_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0o777\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\pathlib.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(pathobj, *args)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\Volumes\\\\D\\\\guohao\\\\resys\\\\dfn\\\\example'"
     ]
    }
   ],
   "source": [
    "main_group_ids=[16,10001,10002,10003,21,10006,10019,10034,20147,20148,10035,20156,61,10047,10048,10049,10050,10055,10056,60, 46, 48, 50, 122]\n",
    "candidate_group_ids=[3060,3061,3062,3063,3064]\n",
    "clicked_group_ids=[3060,3061,3062,3063,3064]\n",
    "unclick_group_ids=[3060,3061,3062,3063,3064]\n",
    "feedback_group_ids=[3060,3061,3063,3064]\n",
    "pos_group_ids=[3065]\n",
    "\n",
    "path = Path(\"/Volumes/D/guohao/resys/dfn/example\")\n",
    "a = next(data_gen(path,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['main_16',\n",
       " 'main_21',\n",
       " 'main_60',\n",
       " 'main_61',\n",
       " 'main_20147',\n",
       " 'main_46',\n",
       " 'main_48',\n",
       " 'main_50',\n",
       " 'main_122',\n",
       " 'main_10002',\n",
       " 'main_10001',\n",
       " 'main_10003',\n",
       " 'main_10006',\n",
       " 'main_10055',\n",
       " 'main_10056',\n",
       " 'main_10048',\n",
       " 'main_20156',\n",
       " 'main_10050',\n",
       " 'main_10034',\n",
       " 'main_10035',\n",
       " 'main_10019',\n",
       " 'main_20148',\n",
       " 'main_10033',\n",
       " 'main_10065',\n",
       " 'main_10049',\n",
       " 'main_10047',\n",
       " 'candidate_3060',\n",
       " 'candidate_3061',\n",
       " 'candidate_3062',\n",
       " 'candidate_3063',\n",
       " 'candidate_3064',\n",
       " 'candidate_3065',\n",
       " 'clicked_0_3060',\n",
       " 'clicked_0_3061',\n",
       " 'clicked_0_3062',\n",
       " 'clicked_0_3063',\n",
       " 'clicked_0_3064',\n",
       " 'clicked_position_0_3065',\n",
       " 'clicked_1_3060',\n",
       " 'clicked_1_3061',\n",
       " 'clicked_1_3062',\n",
       " 'clicked_1_3063',\n",
       " 'clicked_1_3064',\n",
       " 'clicked_position_1_3065',\n",
       " 'clicked_2_3060',\n",
       " 'clicked_2_3061',\n",
       " 'clicked_2_3062',\n",
       " 'clicked_2_3063',\n",
       " 'clicked_2_3064',\n",
       " 'clicked_position_2_3065',\n",
       " 'clicked_3_3060',\n",
       " 'clicked_3_3061',\n",
       " 'clicked_3_3062',\n",
       " 'clicked_3_3063',\n",
       " 'clicked_3_3064',\n",
       " 'clicked_position_3_3065',\n",
       " 'clicked_4_3060',\n",
       " 'clicked_4_3061',\n",
       " 'clicked_4_3062',\n",
       " 'clicked_4_3063',\n",
       " 'clicked_4_3064',\n",
       " 'clicked_position_4_3065',\n",
       " 'clicked_5_3060',\n",
       " 'clicked_5_3061',\n",
       " 'clicked_5_3062',\n",
       " 'clicked_5_3063',\n",
       " 'clicked_5_3064',\n",
       " 'clicked_position_5_3065',\n",
       " 'clicked_6_3060',\n",
       " 'clicked_6_3061',\n",
       " 'clicked_6_3062',\n",
       " 'clicked_6_3063',\n",
       " 'clicked_6_3064',\n",
       " 'clicked_position_6_3065',\n",
       " 'clicked_7_3060',\n",
       " 'clicked_7_3061',\n",
       " 'clicked_7_3062',\n",
       " 'clicked_7_3063',\n",
       " 'clicked_7_3064',\n",
       " 'clicked_position_7_3065',\n",
       " 'clicked_8_3060',\n",
       " 'clicked_8_3061',\n",
       " 'clicked_8_3062',\n",
       " 'clicked_8_3063',\n",
       " 'clicked_8_3064',\n",
       " 'clicked_position_8_3065',\n",
       " 'clicked_9_3060',\n",
       " 'clicked_9_3061',\n",
       " 'clicked_9_3062',\n",
       " 'clicked_9_3063',\n",
       " 'clicked_9_3064',\n",
       " 'clicked_position_9_3065',\n",
       " 'clicked_10_3060',\n",
       " 'clicked_10_3061',\n",
       " 'clicked_10_3062',\n",
       " 'clicked_10_3063',\n",
       " 'clicked_10_3064',\n",
       " 'clicked_position_10_3065',\n",
       " 'clicked_11_3060',\n",
       " 'clicked_11_3061',\n",
       " 'clicked_11_3062',\n",
       " 'clicked_11_3063',\n",
       " 'clicked_11_3064',\n",
       " 'clicked_position_11_3065',\n",
       " 'clicked_12_3060',\n",
       " 'clicked_12_3061',\n",
       " 'clicked_12_3062',\n",
       " 'clicked_12_3063',\n",
       " 'clicked_12_3064',\n",
       " 'clicked_position_12_3065',\n",
       " 'clicked_13_3060',\n",
       " 'clicked_13_3061',\n",
       " 'clicked_13_3062',\n",
       " 'clicked_13_3063',\n",
       " 'clicked_13_3064',\n",
       " 'clicked_position_13_3065',\n",
       " 'clicked_14_3060',\n",
       " 'clicked_14_3061',\n",
       " 'clicked_14_3062',\n",
       " 'clicked_14_3063',\n",
       " 'clicked_14_3064',\n",
       " 'clicked_position_14_3065',\n",
       " 'clicked_15_3060',\n",
       " 'clicked_15_3061',\n",
       " 'clicked_15_3062',\n",
       " 'clicked_15_3063',\n",
       " 'clicked_15_3064',\n",
       " 'clicked_position_15_3065',\n",
       " 'clicked_16_3060',\n",
       " 'clicked_16_3061',\n",
       " 'clicked_16_3062',\n",
       " 'clicked_16_3063',\n",
       " 'clicked_16_3064',\n",
       " 'clicked_position_16_3065',\n",
       " 'clicked_17_3060',\n",
       " 'clicked_17_3061',\n",
       " 'clicked_17_3062',\n",
       " 'clicked_17_3063',\n",
       " 'clicked_17_3064',\n",
       " 'clicked_position_17_3065',\n",
       " 'clicked_18_3060',\n",
       " 'clicked_18_3061',\n",
       " 'clicked_18_3062',\n",
       " 'clicked_18_3063',\n",
       " 'clicked_18_3064',\n",
       " 'clicked_position_18_3065',\n",
       " 'clicked_19_3060',\n",
       " 'clicked_19_3061',\n",
       " 'clicked_19_3062',\n",
       " 'clicked_19_3063',\n",
       " 'clicked_19_3064',\n",
       " 'clicked_position_19_3065',\n",
       " 'clicked_20_3060',\n",
       " 'clicked_20_3061',\n",
       " 'clicked_20_3062',\n",
       " 'clicked_20_3063',\n",
       " 'clicked_20_3064',\n",
       " 'clicked_position_20_3065',\n",
       " 'clicked_21_3060',\n",
       " 'clicked_21_3061',\n",
       " 'clicked_21_3062',\n",
       " 'clicked_21_3063',\n",
       " 'clicked_21_3064',\n",
       " 'clicked_position_21_3065',\n",
       " 'clicked_22_3060',\n",
       " 'clicked_22_3061',\n",
       " 'clicked_22_3062',\n",
       " 'clicked_22_3063',\n",
       " 'clicked_22_3064',\n",
       " 'clicked_position_22_3065',\n",
       " 'clicked_23_3060',\n",
       " 'clicked_23_3061',\n",
       " 'clicked_23_3062',\n",
       " 'clicked_23_3063',\n",
       " 'clicked_23_3064',\n",
       " 'clicked_position_23_3065',\n",
       " 'clicked_24_3060',\n",
       " 'clicked_24_3061',\n",
       " 'clicked_24_3062',\n",
       " 'clicked_24_3063',\n",
       " 'clicked_24_3064',\n",
       " 'clicked_position_24_3065',\n",
       " 'clicked_25_3060',\n",
       " 'clicked_25_3061',\n",
       " 'clicked_25_3062',\n",
       " 'clicked_25_3063',\n",
       " 'clicked_25_3064',\n",
       " 'clicked_position_25_3065',\n",
       " 'clicked_26_3060',\n",
       " 'clicked_26_3061',\n",
       " 'clicked_26_3062',\n",
       " 'clicked_26_3063',\n",
       " 'clicked_26_3064',\n",
       " 'clicked_position_26_3065',\n",
       " 'clicked_27_3060',\n",
       " 'clicked_27_3061',\n",
       " 'clicked_27_3062',\n",
       " 'clicked_27_3063',\n",
       " 'clicked_27_3064',\n",
       " 'clicked_position_27_3065',\n",
       " 'clicked_28_3060',\n",
       " 'clicked_28_3061',\n",
       " 'clicked_28_3062',\n",
       " 'clicked_28_3063',\n",
       " 'clicked_28_3064',\n",
       " 'clicked_position_28_3065',\n",
       " 'clicked_29_3060',\n",
       " 'clicked_29_3061',\n",
       " 'clicked_29_3062',\n",
       " 'clicked_29_3063',\n",
       " 'clicked_29_3064',\n",
       " 'clicked_position_29_3065',\n",
       " 'clicked_histLen',\n",
       " 'unclick_0_3060',\n",
       " 'unclick_0_3061',\n",
       " 'unclick_0_3062',\n",
       " 'unclick_0_3063',\n",
       " 'unclick_0_3064',\n",
       " 'unclick_position_0_3065',\n",
       " 'unclick_1_3060',\n",
       " 'unclick_1_3061',\n",
       " 'unclick_1_3062',\n",
       " 'unclick_1_3063',\n",
       " 'unclick_1_3064',\n",
       " 'unclick_position_1_3065',\n",
       " 'unclick_2_3060',\n",
       " 'unclick_2_3061',\n",
       " 'unclick_2_3062',\n",
       " 'unclick_2_3063',\n",
       " 'unclick_2_3064',\n",
       " 'unclick_position_2_3065',\n",
       " 'unclick_3_3060',\n",
       " 'unclick_3_3061',\n",
       " 'unclick_3_3062',\n",
       " 'unclick_3_3063',\n",
       " 'unclick_3_3064',\n",
       " 'unclick_position_3_3065',\n",
       " 'unclick_4_3060',\n",
       " 'unclick_4_3061',\n",
       " 'unclick_4_3062',\n",
       " 'unclick_4_3063',\n",
       " 'unclick_4_3064',\n",
       " 'unclick_position_4_3065',\n",
       " 'unclick_5_3060',\n",
       " 'unclick_5_3061',\n",
       " 'unclick_5_3062',\n",
       " 'unclick_5_3063',\n",
       " 'unclick_5_3064',\n",
       " 'unclick_position_5_3065',\n",
       " 'unclick_6_3060',\n",
       " 'unclick_6_3061',\n",
       " 'unclick_6_3062',\n",
       " 'unclick_6_3063',\n",
       " 'unclick_6_3064',\n",
       " 'unclick_position_6_3065',\n",
       " 'unclick_7_3060',\n",
       " 'unclick_7_3061',\n",
       " 'unclick_7_3062',\n",
       " 'unclick_7_3063',\n",
       " 'unclick_7_3064',\n",
       " 'unclick_position_7_3065',\n",
       " 'unclick_8_3060',\n",
       " 'unclick_8_3061',\n",
       " 'unclick_8_3062',\n",
       " 'unclick_8_3063',\n",
       " 'unclick_8_3064',\n",
       " 'unclick_position_8_3065',\n",
       " 'unclick_9_3060',\n",
       " 'unclick_9_3061',\n",
       " 'unclick_9_3062',\n",
       " 'unclick_9_3063',\n",
       " 'unclick_9_3064',\n",
       " 'unclick_position_9_3065',\n",
       " 'unclick_10_3060',\n",
       " 'unclick_10_3061',\n",
       " 'unclick_10_3062',\n",
       " 'unclick_10_3063',\n",
       " 'unclick_10_3064',\n",
       " 'unclick_position_10_3065',\n",
       " 'unclick_11_3060',\n",
       " 'unclick_11_3061',\n",
       " 'unclick_11_3062',\n",
       " 'unclick_11_3063',\n",
       " 'unclick_11_3064',\n",
       " 'unclick_position_11_3065',\n",
       " 'unclick_12_3060',\n",
       " 'unclick_12_3061',\n",
       " 'unclick_12_3062',\n",
       " 'unclick_12_3063',\n",
       " 'unclick_12_3064',\n",
       " 'unclick_position_12_3065',\n",
       " 'unclick_13_3060',\n",
       " 'unclick_13_3061',\n",
       " 'unclick_13_3062',\n",
       " 'unclick_13_3063',\n",
       " 'unclick_13_3064',\n",
       " 'unclick_position_13_3065',\n",
       " 'unclick_14_3060',\n",
       " 'unclick_14_3061',\n",
       " 'unclick_14_3062',\n",
       " 'unclick_14_3063',\n",
       " 'unclick_14_3064',\n",
       " 'unclick_position_14_3065',\n",
       " 'unclick_15_3060',\n",
       " 'unclick_15_3061',\n",
       " 'unclick_15_3062',\n",
       " 'unclick_15_3063',\n",
       " 'unclick_15_3064',\n",
       " 'unclick_position_15_3065',\n",
       " 'unclick_16_3060',\n",
       " 'unclick_16_3061',\n",
       " 'unclick_16_3062',\n",
       " 'unclick_16_3063',\n",
       " 'unclick_16_3064',\n",
       " 'unclick_position_16_3065',\n",
       " 'unclick_17_3060',\n",
       " 'unclick_17_3061',\n",
       " 'unclick_17_3062',\n",
       " 'unclick_17_3063',\n",
       " 'unclick_17_3064',\n",
       " 'unclick_position_17_3065',\n",
       " 'unclick_18_3060',\n",
       " 'unclick_18_3061',\n",
       " 'unclick_18_3062',\n",
       " 'unclick_18_3063',\n",
       " 'unclick_18_3064',\n",
       " 'unclick_position_18_3065',\n",
       " 'unclick_19_3060',\n",
       " 'unclick_19_3061',\n",
       " 'unclick_19_3062',\n",
       " 'unclick_19_3063',\n",
       " 'unclick_19_3064',\n",
       " 'unclick_position_19_3065',\n",
       " 'unclick_20_3060',\n",
       " 'unclick_20_3061',\n",
       " 'unclick_20_3062',\n",
       " 'unclick_20_3063',\n",
       " 'unclick_20_3064',\n",
       " 'unclick_position_20_3065',\n",
       " 'unclick_21_3060',\n",
       " 'unclick_21_3061',\n",
       " 'unclick_21_3062',\n",
       " 'unclick_21_3063',\n",
       " 'unclick_21_3064',\n",
       " 'unclick_position_21_3065',\n",
       " 'unclick_22_3060',\n",
       " 'unclick_22_3061',\n",
       " 'unclick_22_3062',\n",
       " 'unclick_22_3063',\n",
       " 'unclick_22_3064',\n",
       " 'unclick_position_22_3065',\n",
       " 'unclick_23_3060',\n",
       " 'unclick_23_3061',\n",
       " 'unclick_23_3062',\n",
       " 'unclick_23_3063',\n",
       " 'unclick_23_3064',\n",
       " 'unclick_position_23_3065',\n",
       " 'unclick_24_3060',\n",
       " 'unclick_24_3061',\n",
       " 'unclick_24_3062',\n",
       " 'unclick_24_3063',\n",
       " 'unclick_24_3064',\n",
       " 'unclick_position_24_3065',\n",
       " 'unclick_25_3060',\n",
       " 'unclick_25_3061',\n",
       " 'unclick_25_3062',\n",
       " 'unclick_25_3063',\n",
       " 'unclick_25_3064',\n",
       " 'unclick_position_25_3065',\n",
       " 'unclick_26_3060',\n",
       " 'unclick_26_3061',\n",
       " 'unclick_26_3062',\n",
       " 'unclick_26_3063',\n",
       " 'unclick_26_3064',\n",
       " 'unclick_position_26_3065',\n",
       " 'unclick_27_3060',\n",
       " 'unclick_27_3061',\n",
       " 'unclick_27_3062',\n",
       " 'unclick_27_3063',\n",
       " 'unclick_27_3064',\n",
       " 'unclick_position_27_3065',\n",
       " 'unclick_28_3060',\n",
       " 'unclick_28_3061',\n",
       " 'unclick_28_3062',\n",
       " 'unclick_28_3063',\n",
       " 'unclick_28_3064',\n",
       " 'unclick_position_28_3065',\n",
       " 'unclick_29_3060',\n",
       " 'unclick_29_3061',\n",
       " 'unclick_29_3062',\n",
       " 'unclick_29_3063',\n",
       " 'unclick_29_3064',\n",
       " 'unclick_position_29_3065',\n",
       " 'unclick_histLen',\n",
       " 'feedback_0_3060',\n",
       " 'feedback_0_3061',\n",
       " 'feedback_0_3063',\n",
       " 'feedback_0_3064',\n",
       " 'feedback_position_0_3065',\n",
       " 'feedback_1_3060',\n",
       " 'feedback_1_3061',\n",
       " 'feedback_1_3063',\n",
       " 'feedback_1_3064',\n",
       " 'feedback_position_1_3065',\n",
       " 'feedback_2_3060',\n",
       " 'feedback_2_3061',\n",
       " 'feedback_2_3063',\n",
       " 'feedback_2_3064',\n",
       " 'feedback_position_2_3065',\n",
       " 'feedback_3_3060',\n",
       " 'feedback_3_3061',\n",
       " 'feedback_3_3063',\n",
       " 'feedback_3_3064',\n",
       " 'feedback_position_3_3065',\n",
       " 'feedback_4_3060',\n",
       " 'feedback_4_3061',\n",
       " 'feedback_4_3063',\n",
       " 'feedback_4_3064',\n",
       " 'feedback_position_4_3065',\n",
       " 'feedback_5_3060',\n",
       " 'feedback_5_3061',\n",
       " 'feedback_5_3063',\n",
       " 'feedback_5_3064',\n",
       " 'feedback_position_5_3065',\n",
       " 'feedback_6_3060',\n",
       " 'feedback_6_3061',\n",
       " 'feedback_6_3063',\n",
       " 'feedback_6_3064',\n",
       " 'feedback_position_6_3065',\n",
       " 'feedback_7_3060',\n",
       " 'feedback_7_3061',\n",
       " 'feedback_7_3063',\n",
       " 'feedback_7_3064',\n",
       " 'feedback_position_7_3065',\n",
       " 'feedback_8_3060',\n",
       " 'feedback_8_3061',\n",
       " 'feedback_8_3063',\n",
       " 'feedback_8_3064',\n",
       " 'feedback_position_8_3065',\n",
       " 'feedback_9_3060',\n",
       " 'feedback_9_3061',\n",
       " 'feedback_9_3063',\n",
       " 'feedback_9_3064',\n",
       " 'feedback_position_9_3065',\n",
       " 'feedback_10_3060',\n",
       " 'feedback_10_3061',\n",
       " 'feedback_10_3063',\n",
       " 'feedback_10_3064',\n",
       " 'feedback_position_10_3065',\n",
       " 'feedback_11_3060',\n",
       " 'feedback_11_3061',\n",
       " 'feedback_11_3063',\n",
       " 'feedback_11_3064',\n",
       " 'feedback_position_11_3065',\n",
       " 'feedback_12_3060',\n",
       " 'feedback_12_3061',\n",
       " 'feedback_12_3063',\n",
       " 'feedback_12_3064',\n",
       " 'feedback_position_12_3065',\n",
       " 'feedback_13_3060',\n",
       " 'feedback_13_3061',\n",
       " 'feedback_13_3063',\n",
       " 'feedback_13_3064',\n",
       " 'feedback_position_13_3065',\n",
       " 'feedback_14_3060',\n",
       " 'feedback_14_3061',\n",
       " 'feedback_14_3063',\n",
       " 'feedback_14_3064',\n",
       " 'feedback_position_14_3065',\n",
       " 'feedback_15_3060',\n",
       " 'feedback_15_3061',\n",
       " 'feedback_15_3063',\n",
       " 'feedback_15_3064',\n",
       " 'feedback_position_15_3065',\n",
       " 'feedback_16_3060',\n",
       " 'feedback_16_3061',\n",
       " 'feedback_16_3063',\n",
       " 'feedback_16_3064',\n",
       " 'feedback_position_16_3065',\n",
       " 'feedback_17_3060',\n",
       " 'feedback_17_3061',\n",
       " 'feedback_17_3063',\n",
       " 'feedback_17_3064',\n",
       " 'feedback_position_17_3065',\n",
       " 'feedback_18_3060',\n",
       " 'feedback_18_3061',\n",
       " 'feedback_18_3063',\n",
       " 'feedback_18_3064',\n",
       " 'feedback_position_18_3065',\n",
       " 'feedback_19_3060',\n",
       " 'feedback_19_3061',\n",
       " 'feedback_19_3063',\n",
       " 'feedback_19_3064',\n",
       " 'feedback_position_19_3065',\n",
       " 'feedback_20_3060',\n",
       " 'feedback_20_3061',\n",
       " 'feedback_20_3063',\n",
       " 'feedback_20_3064',\n",
       " 'feedback_position_20_3065',\n",
       " 'feedback_21_3060',\n",
       " 'feedback_21_3061',\n",
       " 'feedback_21_3063',\n",
       " 'feedback_21_3064',\n",
       " 'feedback_position_21_3065',\n",
       " 'feedback_22_3060',\n",
       " 'feedback_22_3061',\n",
       " 'feedback_22_3063',\n",
       " 'feedback_22_3064',\n",
       " 'feedback_position_22_3065',\n",
       " 'feedback_23_3060',\n",
       " 'feedback_23_3061',\n",
       " 'feedback_23_3063',\n",
       " 'feedback_23_3064',\n",
       " 'feedback_position_23_3065',\n",
       " 'feedback_24_3060',\n",
       " 'feedback_24_3061',\n",
       " 'feedback_24_3063',\n",
       " 'feedback_24_3064',\n",
       " 'feedback_position_24_3065',\n",
       " 'feedback_25_3060',\n",
       " 'feedback_25_3061',\n",
       " 'feedback_25_3063',\n",
       " 'feedback_25_3064',\n",
       " 'feedback_position_25_3065',\n",
       " 'feedback_26_3060',\n",
       " 'feedback_26_3061',\n",
       " 'feedback_26_3063',\n",
       " 'feedback_26_3064',\n",
       " 'feedback_position_26_3065',\n",
       " 'feedback_27_3060',\n",
       " 'feedback_27_3061',\n",
       " 'feedback_27_3063',\n",
       " 'feedback_27_3064',\n",
       " 'feedback_position_27_3065',\n",
       " 'feedback_28_3060',\n",
       " 'feedback_28_3061',\n",
       " 'feedback_28_3063',\n",
       " 'feedback_28_3064',\n",
       " 'feedback_position_28_3065',\n",
       " 'feedback_29_3060',\n",
       " 'feedback_29_3061',\n",
       " 'feedback_29_3063',\n",
       " 'feedback_29_3064',\n",
       " 'feedback_position_29_3065',\n",
       " 'feedback_histLen']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 1048573), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(a[0][\"clicked_position_17_3065\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "attQ_w = tf.Variable(tf.keras.initializers.GlorotNormal()(shape=[100, 4]), name=\"a\" + \"attQ_w\" + str(1), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "batch_size = 960\n",
    "hist_size = 30\n",
    "data_dict = {}\n",
    "feed_dict = {}\n",
    "batch_idx = 0\n",
    "feature_size = 1048573\n",
    "epoch = 25\n",
    "\n",
    "def data_set(data_dict, feature, string):\n",
    "        if string not in data_dict:\n",
    "             data_dict[string] =[[feature]]\n",
    "        else:\n",
    "             if(len(data_dict[string]) < batch_idx + 1):\n",
    "                 data_dict[string].append([feature])\n",
    "             else:\n",
    "                 data_dict[string][batch_idx].append(feature)\n",
    "\n",
    "def input_data_set(data_dict, features, prefix=\"\"):\n",
    "    global main_group_ids, candidate_group_ids\n",
    "    for feature in features:\n",
    "        feature = feature.split(\":\")\n",
    "        feature = int(feature[0])\n",
    "        group_id = feature >> 48\n",
    "        feature = feature % feature_size\n",
    "        if prefix == \"main_\":\n",
    "            if group_id not in main_group_ids:\n",
    "                continue\n",
    "        elif prefix == \"candidate_\":\n",
    "            if group_id not in candidate_group_ids:\n",
    "                continue\n",
    "        data_set(data_dict, feature, prefix+str(group_id))\n",
    "\n",
    "def input_hist_data_set(data_dict, hist_features, hist_group_ids, pos_group_ids, hist_size, prefix=\"\"):\n",
    "    hist_len = len(hist_features)\n",
    "    if hist_features[0] == '\\n' or hist_features[0] == '' or hist_features[0] == ' ':\n",
    "          hist_len = 0\n",
    "    for i in range(0, hist_size):\n",
    "        if i < hist_len:\n",
    "            features = hist_features[i].split()\n",
    "            for feature in features:\n",
    "                 feature = feature.split(\":\")\n",
    "                 feature = int(feature[0])\n",
    "                 group_id = feature >> 48\n",
    "                 feature = feature % feature_size\n",
    "                 if group_id in pos_group_ids:\n",
    "                       data_set(data_dict, feature, prefix+\"position_\"+str(i)+\"_\"+str(group_id))\n",
    "                 else:\n",
    "                       data_set(data_dict, feature, prefix+str(i)+\"_\"+str(group_id))\n",
    "        else:\n",
    "            for group_id in hist_group_ids:\n",
    "                 data_set(data_dict, 0, prefix+str(i)+\"_\"+str(group_id))\n",
    "            for group_id in pos_group_ids:\n",
    "                 data_set(data_dict, 0, prefix+\"position_\"+str(i)+\"_\"+str(group_id))\n",
    "             \n",
    "    if prefix+\"histLen\" not in data_dict:\n",
    "            data_dict[prefix+\"histLen\"] = [hist_len]\n",
    "    else:\n",
    "            data_dict[prefix+\"histLen\"].append(hist_len)\n",
    "\n",
    "def data_dict_sparse_feature(data_dict, string, dtype):\n",
    "    index, value = [], []\n",
    "#     rows, cols, value = [], [], []\n",
    "    for i in range(batch_size):\n",
    "           for k in range(len(data_dict[string][i])):\n",
    "#                 rows.append(i)\n",
    "#                 cols.append(k)\n",
    "                index.append(np.array([i, k], dtype = np.int32))\n",
    "                value.append(data_dict[string][i][k])\n",
    "#     iv = sp.sparse.coo_matrix((value, (rows, cols)), shape=[len(data_dict[string]), feature_size])\n",
    "#     if dtype == tf.int32:\n",
    "#         iv = iv.astype(np.int32)\n",
    "#     elif dtype == tf.float32:\n",
    "#         iv = iv.astype(np.float32)\n",
    "    iv = tf.sparse.SparseTensor(index, value, [len(data_dict[string]), feature_size])\n",
    "    iv = tf.cast(iv, dtype=dtype)\n",
    "    data_dict[string] = iv\n",
    "\n",
    "\n",
    "def train_data_process(data, data_dict, main_group_ids, candidate_group_ids, clicked_group_ids, unclick_group_ids, feedback_group_ids, pos_group_ids):\n",
    "    data = data.split('\\t')\n",
    "    label = float(data[0])\n",
    "    weight = float(data[1])\n",
    "    features = data[2].split('|')\n",
    "    main_features = features[0].split()\n",
    "    candidate_features = features[1].split()\n",
    "    clicked_features = features[2].split(';')\n",
    "    unclick_features = features[3].split(';')\n",
    "    feedback_features = features[4].split(';')\n",
    "    if \"label\" not in data_dict:\n",
    "        data_dict[\"label\"] = [label]\n",
    "    else:\n",
    "        data_dict[\"label\"].append(label)\n",
    "    \n",
    "    if \"weight\" not in data_dict:\n",
    "        data_dict[\"weight\"] = [weight]\n",
    "    else:\n",
    "        data_dict[\"weight\"].append(weight)\n",
    "    \n",
    "    input_data_set(data_dict, main_features, \"main_\")\n",
    "    input_data_set(data_dict, candidate_features, \"candidate_\")\n",
    "    input_hist_data_set(data_dict, clicked_features, clicked_group_ids, pos_group_ids, hist_size, \"clicked_\")\n",
    "    input_hist_data_set(data_dict, unclick_features, unclick_group_ids, pos_group_ids, hist_size, \"unclick_\")\n",
    "    input_hist_data_set(data_dict, feedback_features, feedback_group_ids, pos_group_ids, hist_size, \"feedback_\")\n",
    "\n",
    "\n",
    "def data_gen(path):\n",
    "    global batch_idx, data_dict, batch_size, main_group_ids, candidate_group_ids, clicked_group_ids, unclick_group_ids, feedback_group_ids, pos_group_ids\n",
    "    while True:\n",
    "        f = path.open(mode='r')\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            train_data_process(line, data_dict, main_group_ids, candidate_group_ids, clicked_group_ids, unclick_group_ids, feedback_group_ids, pos_group_ids)\n",
    "            if batch_idx < batch_size -1: \n",
    "                batch_idx += 1\n",
    "            else:\n",
    "                for group_id in main_group_ids:\n",
    "                    data_name = \"main_\" + str(group_id)\n",
    "                    data_dict_sparse_feature(data_dict, data_name, tf.int32)\n",
    "                for group_id in candidate_group_ids:\n",
    "                    data_name = \"candidate_\" + str(group_id)\n",
    "                    data_dict_sparse_feature(data_dict, data_name, tf.int32)\n",
    "                for i in range(hist_size):\n",
    "                    for group_id in clicked_group_ids:\n",
    "                        data_name = \"clicked_\" + str(i) + \"_\" + str(group_id)\n",
    "                        data_dict_sparse_feature(data_dict, data_name, tf.int32) \n",
    "                    for group_id in unclick_group_ids:\n",
    "                        data_name = \"unclick_\" + str(i) + \"_\" + str(group_id)\n",
    "                        data_dict_sparse_feature(data_dict, data_name, tf.int32) \n",
    "                    for group_id in feedback_group_ids:\n",
    "                        data_name = \"feedback_\" + str(i) + \"_\" + str(group_id)\n",
    "                        data_dict_sparse_feature(data_dict, data_name, tf.int32)\n",
    "                    for group_id in pos_group_ids:   \n",
    "                        data_name = \"clicked_position_\" + str(i) + \"_\" + str(group_id)\n",
    "                        data_dict_sparse_feature(data_dict, data_name, tf.int32)\n",
    "                        data_name = \"unclick_position_\" + str(i) + \"_\" + str(group_id)\n",
    "                        data_dict_sparse_feature(data_dict, data_name, tf.int32)\n",
    "                        data_name = \"feedback_position_\" + str(i) + \"_\" + str(group_id)\n",
    "                        data_dict_sparse_feature(data_dict, data_name, tf.int32)\n",
    "                data_dict[\"clicked_histLen\"] = tf.convert_to_tensor(data_dict[\"clicked_histLen\"], dtype=tf.float32)\n",
    "                data_dict[\"unclick_histLen\"] = tf.convert_to_tensor(data_dict[\"unclick_histLen\"], dtype=tf.float32)\n",
    "                data_dict[\"feedback_histLen\"] = tf.convert_to_tensor(data_dict[\"feedback_histLen\"], dtype=tf.float32)\n",
    "                data_dict[\"label\"] = tf.convert_to_tensor(data_dict[\"label\"], dtype=tf.float32)\n",
    "                data_dict[\"weight\"] = tf.convert_to_tensor(data_dict[\"weight\"], dtype=tf.float32)\n",
    "                data_input = {k: v for k, v in data_dict.items() if k != \"label\" and k != \"weight\"}\n",
    "                labels = data_dict[\"label\"]\n",
    "                weights = data_dict[\"weight\"]\n",
    "                batch_idx = 0\n",
    "                data_dict = {}\n",
    "                yield (data_input, labels, weights)\n",
    "            line = f.readline()\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_group_ids=[16,10001,10002,10003,21,10006,10019,10034,20147,20148,10035,20156,61,10047,10048,10049,10050,10055,10056,60]\n",
    "candidate_group_ids=[3060,3061,3062,3063,3064]\n",
    "clicked_group_ids=[3060,3061,3062,3063,3064]\n",
    "unclick_group_ids=[3060,3061,3062,3063,3064]\n",
    "feedback_group_ids=[3060,3061,3063,3064]\n",
    "pos_group_ids=[3065]\n",
    "path = Path(r\"E:\\ML_study\\deepctr\\dfn_tf2\\example\")\n",
    "# a = next(data_gen(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, sample_weight = next(data_gen(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build input\n",
    "from collections import OrderedDict\n",
    "group_feature = OrderedDict()\n",
    "for group_id in candidate_group_ids:\n",
    "    group_feature[\"candidate_\" + str(group_id)] = tf.keras.layers.Input(shape=(feature_size, ), dtype=tf.int32, sparse=True, name=(\"candidate_\" + str(group_id)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_Lookup(tf.keras.layers.Layer):\n",
    "    def __init__(self, feature_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_size = feature_size\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        self.embedding_w = self.add_weight(name=\"embedding_w\", shape=(self.feature_size, 16), \n",
    "                                           initializer=tf.keras.initializers.TruncatedNormal(mean=0., stddev=0.01),\n",
    "                                          )\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        embedding = tf.nn.embedding_lookup_sparse(self.embedding_w, inputs, sp_weights=None, combiner='mean')\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_embeddings = []\n",
    "embed_layer = Embedding_Lookup(feature_size)\n",
    "for group_id in candidate_group_ids:\n",
    "    embedding = embed_layer(group_feature[\"candidate_\" + str(group_id)])\n",
    "    candidate_embeddings.append(embedding)\n",
    "candidate_embedding = tf.concat(candidate_embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def embedding_lookup(embedding_w, group_ids, prefix=\"\"):\n",
    "#     embeddings = []\n",
    "#     for group_id in group_ids:\n",
    "#         embedding = tf.nn.embedding_lookup_sparse(embedding_w, group_feature[prefix + str(group_id)], sp_weights=None, combiner='mean')\n",
    "#         embeddings.append(embedding)\n",
    "#     embedding_out = tf.concat(embeddings, axis=1)\n",
    "#     return embedding_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_w = tf.keras.initializers.TruncatedNormal(mean=0., stddev=0.01)\n",
    "# embed_w = tf.Variable(init_w(shape=[feature_size, 16]), name='embedding_w', dtype=tf.float32)\n",
    "\n",
    "# # batch_size, len(main_group_ids) * embed_dim, 相同field之间的特征求mean\n",
    "# candidate_embedding = embedding_lookup(embed_w, candidate_group_ids, prefix=\"candidate_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "output = dense(candidate_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        if len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            x, y = data\n",
    "\n",
    "        y_pred = self(x, training=True)  # Forward pass\n",
    "        # Compute the loss value.\n",
    "        # The loss function is configured in `compile()`.\n",
    "        loss = self.compiled_loss(\n",
    "            y,\n",
    "            y_pred,\n",
    "            sample_weight=sample_weight,\n",
    "            regularization_losses=self.losses,\n",
    "        )\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tf.gradients(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics.\n",
    "        # Metrics are configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel(group_feature, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Model(group_feature, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "Epoch 1/25\n",
      " 16/256 [>.............................] - ETA: 3s"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function len> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_num_elements\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    615\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexedSlices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m   \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`grad` not a Tensor or IndexedSlices.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'NoneType'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-9e735aa85b90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model.compile(tf.keras.optimizers.Adagrad(), \"binary_crossentropy\",\n\u001b[0;32m      2\u001b[0m               metrics=['binary_crossentropy'], )\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    501\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m       \u001b[0minitializer_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObjectIdentityDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    406\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    407\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 408\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2150\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2041\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    913\u001b[0m                                           converted_func)\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[1;34m(input_iterator)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[1;32m---> 73\u001b[1;33m         per_replica_function, args=(model, x, y, sample_weights))\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     all_outputs = dist_utils.unwrap_output_dict(\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m    758\u001b[0m       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\n\u001b[0;32m    759\u001b[0m                                 convert_by_default=False)\n\u001b[1;32m--> 760\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1785\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1787\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2130\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2131\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[1;32m-> 2132\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2134\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    312\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m           \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[0;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_aggregate_grads\u001b[1;34m(gradients)\u001b[0m\n\u001b[0;32m    596\u001b[0m   \u001b[1;32massert\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"No gradients to aggregate\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <built-in function len> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "model.compile(tf.keras.optimizers.Adagrad(), \"binary_crossentropy\",\n",
    "              metrics=['binary_crossentropy'], )\n",
    "history = model.fit(train_data, train_label, epochs=25, batch_size=16, shuffle=False, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset([r\"E:\\ML_study\\deepctr\\dfn_tf2\\example\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data_set(data_dict, features, prefix=\"\"):\n",
    "    global main_group_ids\n",
    "    for feature in features:\n",
    "        feature = feature.split(\":\")\n",
    "        feature = int(feature[0])\n",
    "        group_id = feature >> 48\n",
    "        feature = feature % feature_size\n",
    "        if prefix == \"main_\":\n",
    "            if group_id not in main_group_ids:\n",
    "                continue             \n",
    "        data_dict[prefix+str(group_id)] =[feature]\n",
    "        index, value = [], []\n",
    "        for k in range(len(data_dict[string][0])):\n",
    "    #                 rows.append(i)\n",
    "    #                 cols.append(k)\n",
    "                    index.append(np.array([i, k], dtype = np.int32))\n",
    "                    value.append(data_dict[string][i][k])\n",
    "    #     iv = sp.sparse.coo_matrix((value, (rows, cols)), shape=[len(data_dict[string]), feature_size])\n",
    "    #     if dtype == tf.int32:\n",
    "    #         iv = iv.astype(np.int32)\n",
    "    #     elif dtype == tf.float32:\n",
    "    #         iv = iv.astype(np.float32)\n",
    "        iv = tf.sparse.SparseTensor(index, value, [len(data_dict[string]), feature_size])\n",
    "        iv = tf.cast(iv, dtype=dtype)\n",
    "        data_dict[string] = iv\n",
    "        \n",
    "def train_data_process(data, data_dict, main_group_ids, candidate_group_ids, clicked_group_ids, unclick_group_ids, feedback_group_ids, pos_group_ids):\n",
    "    data_dict = {}\n",
    "    data = tf.strings.split(data, \"\\t\")\n",
    "    label = tf.cast(data[0], dtype=tf.float32)\n",
    "    weight = tf.cast(data[1], dtype=tf.float32)\n",
    "    features = tf.strings.split(data[2], \"|\")\n",
    "    main_features = tf.strings.split(features[0], \" \")\n",
    "    candidate_features = tf.strings.split(features[1], \" \")\n",
    "    clicked_features = tf.strings.split(features[2], \";\")\n",
    "    unclick_features = tf.strings.split(features[3], \";\")\n",
    "    feedback_features = tf.strings.split(features[4], \";\")\n",
    "    data_dict[\"label\"] = [label]\n",
    "    \n",
    "    data_dict[\"weight\"] = [weight]\n",
    "    \n",
    "    input_data_set(data_dict, main_features, \"main_\")\n",
    "    input_data_set(data_dict, candidate_features, \"candidate_\")\n",
    "    input_hist_data_set(data_dict, clicked_features, clicked_group_ids, pos_group_ids, hist_size, \"clicked_\")\n",
    "    input_hist_data_set(data_dict, unclick_features, unclick_group_ids, pos_group_ids, hist_size, \"unclick_\")\n",
    "    input_hist_data_set(data_dict, feedback_features, feedback_group_ids, pos_group_ids, hist_size, \"feedback_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'0\\t1.0\\t4597697815101895:1.0 6016574026494338:1.0 17027853178969981:1.0 17195487356851262:1.0 5671018667602395793:1.0 13070185260218823:1.0 13042712529216655:1.0 13003510089913431:1.0 13101372349892736:1.0 13102308708812472:1.0 13681236340273332:1.0 13720715262011759:1.0 13742524213838863:1.0 13573771682066531:1.0 13527563666807508:1.0 14335342827392411:1.0 14213538197159935:1.0 14230815525973305:1.0 14250523910490838:1.0 14309043682027648:1.0 34505104869198441:1.0 34606920073409687:1.0 34369972135808970:1.0 34432175036929139:1.0 34574397145101198:1.0 2815540589333415558:1.0 2815310558040711643:1.0 2815845193123668893:1.0 2816472454912085528:1.0 2830453274016811363:1.0 2830604277019819904:1.0 2828345537882128037:1.0 5673538572898786380:1.0 2829095378905735861:1.0 2824466694240443953:1.0 2824658891131706915:1.0 2820179654887781996:1.0 2820283407214145740:1.0 2820107523379246349:1.0 2820294557960302561:1.0 2820246555415114990:1.0 2820232643767827587:1.0 2820182182810824006:1.0 2820348817245503466:1.0 2820351480220271624:1.0 2820280157049914762:1.0 2820209548693929027:1.0 2820310163084641160:1.0 2820215669636338215:1.0 2820260677638951423:1.0 5671303724387606278:1.0 5671349177591457642:1.0 5671254970983595692:1.0 5671382982090538810:1.0 5671331174452782131:1.0 5671262954985621098:1.0 5671364081527634756:1.0 5671281405143200973:1.0 5671316343381455137:1.0 5671171489271785968:1.0 5671383433951866090:1.0 5671285266247476982:1.0 5671198137458288753:1.0 2824293295427244245:1.0 2824313793729891575:1.0 2824163540603296623:1.0 2824127288288134198:1.0 2824063880585922057:1.0 2824268306818755435:1.0 2824209969052691577:1.0 2824072756564822021:1.0 2833320024707657974:1.0 2833322551155159461:1.0 2833195907218100923:1.0 2833052981574243595:1.0 2833218300981980073:1.0 2833122500872904185:1.0 2833118849327686063:1.0 2828629163875481879:1.0 2828044804560022748:1.0|861388334503892902:1.0 861861330773702844:1.0 862136269045099785:1.0 862266905517130820:1.0 862501614802454984:1.0 862810221768640946:1.0|861577123031706908:1.0 861769181515802583:1.0 861881058815502557:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861495099862856553:1.0 861615947801238077:1.0 862127627275336382:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861556016944150930:1.0 861767744450805936:1.0 862012258868493194:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861569356112903405:1.0 861822228743483476:1.0 861950394866561113:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861414839333611708:1.0 861653716267376982:1.0 861973681977107327:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861464220456366215:1.0 861620796433787502:1.0 862029838927705188:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861395749550093080:1.0 861815418441092222:1.0 862022073955905718:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0;861490385063769284:1.0 861739388185793066:1.0 862144415389697402:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0|861519593336408472:1.0 861835802402008686:1.0 862110738087230307:1.0 862361608246017547:1.0 862610971854242761:1.0 862899130587065188:1.0;861392626822994499:1.0 861676119560622373:1.0 861934181556817485:1.0 862391853785157069:1.0 862588479281355076:1.0 862899130587065188:1.0;861578494207749664:1.0 861799326538603480:1.0 861877520699208188:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0;861434357586189860:1.0 861646188003329645:1.0 862032145175070666:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0;861502224075656678:1.0 861868527776954929:1.0 861950394866561113:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861434025215191534:1.0 861833556046309559:1.0 862034425123780065:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861458051457848172:1.0 861859206185607015:1.0 862011061326961450:1.0 862438218285668697:1.0 862517285208108214:1.0 862899130587065188:1.0;861505175510885401:1.0 861748843453638028:1.0 861878459046995469:1.0 862266905517130820:1.0 862482319750131362:1.0 862899130587065188:1.0;861457080689820141:1.0 861802504276859133:1.0 861969585531765177:1.0 862247359178082238:1.0 862525223368733971:1.0 862899130587065188:1.0;861453434146726909:1.0 861608383232980488:1.0 862105397134285933:1.0 862391853785157069:1.0 862588479281355076:1.0 862899130587065188:1.0;861314025976198703:1.0 861787646589033297:1.0 862080861605248687:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0;861387325012980568:1.0 861618566743921695:1.0 861908964966161203:1.0 862438218285668697:1.0 862517285208108214:1.0 862899130587065188:1.0;861466315609639829:1.0 861765326728784475:1.0 862057297027417913:1.0 862385730517928461:1.0 862443777178067152:1.0 862899130587065188:1.0;861523804510012835:1.0 861813430952576346:1.0 862035631823603456:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0;861370504659110060:1.0 861673413812676965:1.0 861902649919633795:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861572597071006726:1.0 861827931372330329:1.0 862012833782644024:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861524179584886857:1.0 861734506404850637:1.0 861993183080966178:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861397471342818266:1.0 861757238306588305:1.0 861971210899440333:1.0 862206651961373594:1.0 862521557091614424:1.0 862899130587065188:1.0;861538230115124050:1.0 861606082903012233:1.0 862057847764726680:1.0 862259049287616939:1.0 862514935708808651:1.0 862899130587065188:1.0;861388176907487321:1.0 861797186533421635:1.0 862024418536767103:1.0 862432239387138417:1.0 862443722727701870:1.0 862899130587065188:1.0;861483537440116841:1.0 861846090543303518:1.0 861913260461374783:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861478754979462184:1.0 861823266456813579:1.0 861995437147326025:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0;861502477039093510:1.0 861865434651925439:1.0 861903999771275836:1.0 862266905517130820:1.0 862523560904741743:1.0 862899130587065188:1.0|'\n",
      "b'0\\t1.0\\t4597697815101895:1.0 6016574026494338:1.0 17027853178969981:1.0 17195487356851262:1.0 5671018667602395793:1.0 13070185260218823:1.0 13042712529216655:1.0 13003510089913431:1.0 13101372349892736:1.0 13102308708812472:1.0 13681236340273332:1.0 13720715262011759:1.0 13742524213838863:1.0 13573771682066531:1.0 13527563666807508:1.0 14335342827392411:1.0 14213538197159935:1.0 14230815525973305:1.0 14250523910490838:1.0 14309043682027648:1.0 34505104869198441:1.0 34606920073409687:1.0 34369972135808970:1.0 34432175036929139:1.0 34574397145101198:1.0 2815581801380901125:1.0 2815301649996531051:1.0 2815761522496155560:1.0 2816604925348472055:1.0 2830383249215707469:1.0 2830604277019819904:1.0 2828345537882128037:1.0 5673549968350794341:1.0 2829095378905735861:1.0 2824467147184334429:1.0 2824870812332339795:1.0 2820225094126542404:1.0 2820334589588684103:1.0 2820364067923308032:1.0 2820245444954230166:1.0 2820202736857784676:1.0 2820262095225828678:1.0 2820365488762530978:1.0 2820121206270164417:1.0 2820284433111916247:1.0 2820133827931054974:1.0 2820286177457704521:1.0 2820112482784469526:1.0 2820313044670912052:1.0 2820290746940159584:1.0 2820264051177745649:1.0 2820361209814026712:1.0 2820254746476547389:1.0 2820182189269299162:1.0 5671173119451365704:1.0 5671401884629500115:1.0 5671219706049745348:1.0 5671262976672840709:1.0 5671410113639720581:1.0 5671171485684041710:1.0 5671370362943936450:1.0 5671172978072122001:1.0 5671311915845414631:1.0 5671308138070168095:1.0 5671435726770038726:1.0 5671336237996545187:1.0 5671291064420717553:1.0 5671343409851076741:1.0 5671244990731354001:1.0 5671293868689556992:1.0 5671306500165256140:1.0 2824168501077009246:1.0 2824318422094035107:1.0 2824221774504948073:1.0 2824192955574845571:1.0 2824158857309958783:1.0 2824125518359203125:1.0 2833111417030475154:1.0 2833219729276280487:1.0 2833097622045332601:1.0 2833317618366662021:1.0 2833181325394059327:1.0 2828630366570843767:1.0 2828051930026006904:1.0|861534241222939480:1.0 861781206507797428:1.0 862100122630536264:1.0 862247359178082238:1.0 862525223368733971:1.0 862810221768640946:1.0|861577123031706908:1.0 861769181515802583:1.0 861881058815502557:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861495099862856553:1.0 861615947801238077:1.0 862127627275336382:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861556016944150930:1.0 861767744450805936:1.0 862012258868493194:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861569356112903405:1.0 861822228743483476:1.0 861950394866561113:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861414839333611708:1.0 861653716267376982:1.0 861973681977107327:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861464220456366215:1.0 861620796433787502:1.0 862029838927705188:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861395749550093080:1.0 861815418441092222:1.0 862022073955905718:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0;861490385063769284:1.0 861739388185793066:1.0 862144415389697402:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0|861519593336408472:1.0 861835802402008686:1.0 862110738087230307:1.0 862361608246017547:1.0 862610971854242761:1.0 862899130587065188:1.0;861392626822994499:1.0 861676119560622373:1.0 861934181556817485:1.0 862391853785157069:1.0 862588479281355076:1.0 862899130587065188:1.0;861578494207749664:1.0 861799326538603480:1.0 861877520699208188:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0;861434357586189860:1.0 861646188003329645:1.0 862032145175070666:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0;861502224075656678:1.0 861868527776954929:1.0 861950394866561113:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861434025215191534:1.0 861833556046309559:1.0 862034425123780065:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861458051457848172:1.0 861859206185607015:1.0 862011061326961450:1.0 862438218285668697:1.0 862517285208108214:1.0 862899130587065188:1.0;861505175510885401:1.0 861748843453638028:1.0 861878459046995469:1.0 862266905517130820:1.0 862482319750131362:1.0 862899130587065188:1.0;861457080689820141:1.0 861802504276859133:1.0 861969585531765177:1.0 862247359178082238:1.0 862525223368733971:1.0 862899130587065188:1.0;861453434146726909:1.0 861608383232980488:1.0 862105397134285933:1.0 862391853785157069:1.0 862588479281355076:1.0 862899130587065188:1.0;861314025976198703:1.0 861787646589033297:1.0 862080861605248687:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0;861387325012980568:1.0 861618566743921695:1.0 861908964966161203:1.0 862438218285668697:1.0 862517285208108214:1.0 862899130587065188:1.0;861466315609639829:1.0 861765326728784475:1.0 862057297027417913:1.0 862385730517928461:1.0 862443777178067152:1.0 862899130587065188:1.0;861523804510012835:1.0 861813430952576346:1.0 862035631823603456:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0;861370504659110060:1.0 861673413812676965:1.0 861902649919633795:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861572597071006726:1.0 861827931372330329:1.0 862012833782644024:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861524179584886857:1.0 861734506404850637:1.0 861993183080966178:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861397471342818266:1.0 861757238306588305:1.0 861971210899440333:1.0 862206651961373594:1.0 862521557091614424:1.0 862899130587065188:1.0;861538230115124050:1.0 861606082903012233:1.0 862057847764726680:1.0 862259049287616939:1.0 862514935708808651:1.0 862899130587065188:1.0;861388176907487321:1.0 861797186533421635:1.0 862024418536767103:1.0 862432239387138417:1.0 862443722727701870:1.0 862899130587065188:1.0;861483537440116841:1.0 861846090543303518:1.0 861913260461374783:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861478754979462184:1.0 861823266456813579:1.0 861995437147326025:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0;861502477039093510:1.0 861865434651925439:1.0 861903999771275836:1.0 862266905517130820:1.0 862523560904741743:1.0 862899130587065188:1.0|'\n",
      "b'0\\t1.0\\t4723242577220648:1.0 6024832381639673:1.0 17027853178969981:1.0 17195487356851262:1.0 5671018667602395793:1.0 13070185260218823:1.0 13042712529216655:1.0 13003510089913431:1.0 13101372349892736:1.0 13102308708812472:1.0 13681236340273332:1.0 13720715262011759:1.0 13742524213838863:1.0 13573771682066531:1.0 13527563666807508:1.0 14335342827392411:1.0 14213538197159935:1.0 14230815525973305:1.0 14250523910490838:1.0 14309043682027648:1.0 34505104869198441:1.0 34606920073409687:1.0 34369972135808970:1.0 34432175036929139:1.0 34574397145101198:1.0 2815467632218454873:1.0 2815032785258155242:1.0 2815845193123668893:1.0 2816497379529559714:1.0 2830300805155462371:1.0 2830604277019819904:1.0 2828406761363638128:1.0 5673579569797239185:1.0 2829095378905735861:1.0 2824403248342612973:1.0 2824653528938015983:1.0 2820241584808655620:1.0 2820208299704962728:1.0 2820244081587245998:1.0 2820240282392888042:1.0 2820267084940758979:1.0 2820158070399477061:1.0 2820214505378552570:1.0 5671407949962228864:1.0 5671236005418780000:1.0 5671349199459078956:1.0 5671171180871626344:1.0 5671194275559044206:1.0 5671277178919188413:1.0 2828746164096730080:1.0 2828008350199741411:1.0|861346024182566845:1.0 861734254471596097:1.0 862029234321405340:1.0 862266905517130820:1.0 862581761269226550:1.0 862810221768640946:1.0|861577123031706908:1.0 861769181515802583:1.0 861881058815502557:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861495099862856553:1.0 861615947801238077:1.0 862127627275336382:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861556016944150930:1.0 861767744450805936:1.0 862012258868493194:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861569356112903405:1.0 861822228743483476:1.0 861950394866561113:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861414839333611708:1.0 861653716267376982:1.0 861973681977107327:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861464220456366215:1.0 861620796433787502:1.0 862029838927705188:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861395749550093080:1.0 861815418441092222:1.0 862022073955905718:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0|861519593336408472:1.0 861835802402008686:1.0 862110738087230307:1.0 862361608246017547:1.0 862610971854242761:1.0 862899130587065188:1.0;861392626822994499:1.0 861676119560622373:1.0 861934181556817485:1.0 862391853785157069:1.0 862588479281355076:1.0 862899130587065188:1.0;861578494207749664:1.0 861799326538603480:1.0 861877520699208188:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0;861434357586189860:1.0 861646188003329645:1.0 862032145175070666:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0;861502224075656678:1.0 861868527776954929:1.0 861950394866561113:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861434025215191534:1.0 861833556046309559:1.0 862034425123780065:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861458051457848172:1.0 861859206185607015:1.0 862011061326961450:1.0 862438218285668697:1.0 862517285208108214:1.0 862899130587065188:1.0;861505175510885401:1.0 861748843453638028:1.0 861878459046995469:1.0 862266905517130820:1.0 862482319750131362:1.0 862899130587065188:1.0;861457080689820141:1.0 861802504276859133:1.0 861969585531765177:1.0 862247359178082238:1.0 862525223368733971:1.0 862899130587065188:1.0;861453434146726909:1.0 861608383232980488:1.0 862105397134285933:1.0 862391853785157069:1.0 862588479281355076:1.0 862899130587065188:1.0;861314025976198703:1.0 861787646589033297:1.0 862080861605248687:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0;861387325012980568:1.0 861618566743921695:1.0 861908964966161203:1.0 862438218285668697:1.0 862517285208108214:1.0 862899130587065188:1.0;861466315609639829:1.0 861765326728784475:1.0 862057297027417913:1.0 862385730517928461:1.0 862443777178067152:1.0 862899130587065188:1.0;861523804510012835:1.0 861813430952576346:1.0 862035631823603456:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0;861370504659110060:1.0 861673413812676965:1.0 861902649919633795:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861572597071006726:1.0 861827931372330329:1.0 862012833782644024:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861524179584886857:1.0 861734506404850637:1.0 861993183080966178:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861397471342818266:1.0 861757238306588305:1.0 861971210899440333:1.0 862206651961373594:1.0 862521557091614424:1.0 862899130587065188:1.0;861538230115124050:1.0 861606082903012233:1.0 862057847764726680:1.0 862259049287616939:1.0 862514935708808651:1.0 862899130587065188:1.0;861388176907487321:1.0 861797186533421635:1.0 862024418536767103:1.0 862432239387138417:1.0 862443722727701870:1.0 862899130587065188:1.0;861483537440116841:1.0 861846090543303518:1.0 861913260461374783:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861478754979462184:1.0 861823266456813579:1.0 861995437147326025:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0|'\n",
      "b'0\\t1.0\\t4723242577220648:1.0 6024832381639673:1.0 17027853178969981:1.0 17195487356851262:1.0 5671018667602395793:1.0 13070185260218823:1.0 13042712529216655:1.0 13003510089913431:1.0 13101372349892736:1.0 13102308708812472:1.0 13681236340273332:1.0 13720715262011759:1.0 13742524213838863:1.0 13573771682066531:1.0 13527563666807508:1.0 14335342827392411:1.0 14213538197159935:1.0 14230815525973305:1.0 14250523910490838:1.0 14309043682027648:1.0 34505104869198441:1.0 34606920073409687:1.0 34369972135808970:1.0 34432175036929139:1.0 34574397145101198:1.0 2815380080670937769:1.0 2815221068059233504:1.0 2815801890729904879:1.0 2816655962788825937:1.0 2830426512107067313:1.0 2830604277019819904:1.0 2828293940815377667:1.0 5673514879622751325:1.0 2829095378905735861:1.0 2824524785640965967:1.0 2824604845629076145:1.0 2820117944674341917:1.0 2820284453585868101:1.0 2820193438564028968:1.0 2820136225990850451:1.0 2820237511175055902:1.0 2820338724763367385:1.0 2820242409836075136:1.0 2820322803440964937:1.0 2820151911981369338:1.0 2820150727665416281:1.0 2820164914210392785:1.0 2820131878408124445:1.0 2820320927763194427:1.0 2820117575737749897:1.0 2820294212410381780:1.0 2820200710798801525:1.0 2820345276974569609:1.0 2820293973209521887:1.0 5671390566751089652:1.0 5671399651307865436:1.0 5671297669283403127:1.0 5671245171354957634:1.0 5671305526998188458:1.0 5671367552295970723:1.0 5671438113480484139:1.0 5671324539411957855:1.0 5671229370510820359:1.0 5671308477961386427:1.0 5671244479405273797:1.0 5671264719003040028:1.0 5671432097803364795:1.0 5671188184036410229:1.0 5671177981799479116:1.0 5671311381122199181:1.0 5671173017945592691:1.0 2824270626285821623:1.0 2824301363598596517:1.0 2824291726346404210:1.0 2833317963429467388:1.0 2833052169823663120:1.0 2828634743823095352:1.0 2828004689464958343:1.0|861505851601768393:1.0 861837149834513768:1.0 861929981747177426:1.0 862438218285668697:1.0 862624348526786487:1.0 862810221768640946:1.0|861577123031706908:1.0 861769181515802583:1.0 861881058815502557:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861495099862856553:1.0 861615947801238077:1.0 862127627275336382:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861556016944150930:1.0 861767744450805936:1.0 862012258868493194:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861569356112903405:1.0 861822228743483476:1.0 861950394866561113:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861414839333611708:1.0 861653716267376982:1.0 861973681977107327:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861464220456366215:1.0 861620796433787502:1.0 862029838927705188:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861395749550093080:1.0 861815418441092222:1.0 862022073955905718:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0|861519593336408472:1.0 861835802402008686:1.0 862110738087230307:1.0 862361608246017547:1.0 862610971854242761:1.0 862899130587065188:1.0;861392626822994499:1.0 861676119560622373:1.0 861934181556817485:1.0 862391853785157069:1.0 862588479281355076:1.0 862899130587065188:1.0;861578494207749664:1.0 861799326538603480:1.0 861877520699208188:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0;861434357586189860:1.0 861646188003329645:1.0 862032145175070666:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0;861502224075656678:1.0 861868527776954929:1.0 861950394866561113:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861434025215191534:1.0 861833556046309559:1.0 862034425123780065:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861458051457848172:1.0 861859206185607015:1.0 862011061326961450:1.0 862438218285668697:1.0 862517285208108214:1.0 862899130587065188:1.0;861505175510885401:1.0 861748843453638028:1.0 861878459046995469:1.0 862266905517130820:1.0 862482319750131362:1.0 862899130587065188:1.0;861457080689820141:1.0 861802504276859133:1.0 861969585531765177:1.0 862247359178082238:1.0 862525223368733971:1.0 862899130587065188:1.0;861453434146726909:1.0 861608383232980488:1.0 862105397134285933:1.0 862391853785157069:1.0 862588479281355076:1.0 862899130587065188:1.0;861314025976198703:1.0 861787646589033297:1.0 862080861605248687:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0;861387325012980568:1.0 861618566743921695:1.0 861908964966161203:1.0 862438218285668697:1.0 862517285208108214:1.0 862899130587065188:1.0;861466315609639829:1.0 861765326728784475:1.0 862057297027417913:1.0 862385730517928461:1.0 862443777178067152:1.0 862899130587065188:1.0;861523804510012835:1.0 861813430952576346:1.0 862035631823603456:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0;861370504659110060:1.0 861673413812676965:1.0 861902649919633795:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861572597071006726:1.0 861827931372330329:1.0 862012833782644024:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861524179584886857:1.0 861734506404850637:1.0 861993183080966178:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861397471342818266:1.0 861757238306588305:1.0 861971210899440333:1.0 862206651961373594:1.0 862521557091614424:1.0 862899130587065188:1.0;861538230115124050:1.0 861606082903012233:1.0 862057847764726680:1.0 862259049287616939:1.0 862514935708808651:1.0 862899130587065188:1.0;861388176907487321:1.0 861797186533421635:1.0 862024418536767103:1.0 862432239387138417:1.0 862443722727701870:1.0 862899130587065188:1.0;861483537440116841:1.0 861846090543303518:1.0 861913260461374783:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861478754979462184:1.0 861823266456813579:1.0 861995437147326025:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0|'\n",
      "b'0\\t1.0\\t4525503193324468:1.0 6024832381639673:1.0 17027853178969981:1.0 17195487356851262:1.0 5671018667602395793:1.0 13070185260218823:1.0 13042712529216655:1.0 13003510089913431:1.0 13101372349892736:1.0 13102308708812472:1.0 13681236340273332:1.0 13720715262011759:1.0 13742524213838863:1.0 13573771682066531:1.0 13527563666807508:1.0 14335342827392411:1.0 14213538197159935:1.0 14230815525973305:1.0 14250523910490838:1.0 14309043682027648:1.0 34505104869198441:1.0 34606920073409687:1.0 34369972135808970:1.0 34432175036929139:1.0 34574397145101198:1.0 2815316152900452696:1.0 2815142471868396185:1.0 2815761522496155560:1.0 2816655962788825937:1.0 2830426512107067313:1.0 2830604277019819904:1.0 2828345537882128037:1.0 5673569444353179269:1.0 2829095378905735861:1.0 2824524785640965967:1.0 2824853008393072204:1.0 2820125818549098801:1.0 2820278105827580480:1.0 2820206323217390435:1.0 2820210506610819090:1.0 2820195890214983321:1.0 2820220450537324731:1.0 2820206323217390435:1.0 2820378017612569504:1.0 2820170767995505365:1.0 2820284433111916247:1.0 2820295138889907953:1.0 2820107763665645049:1.0 2820229100921579272:1.0 2820142548286973832:1.0 5671371563261186547:1.0 5671193782923058436:1.0 5671171719093378184:1.0 5671305125375177129:1.0 5671313462717864899:1.0 5671400397166213299:1.0 5671298411532018640:1.0 5671286259179139249:1.0 5671365922017585527:1.0 5671181824458921641:1.0 5671324988435529160:1.0 5671234861695525297:1.0 5671394588662655449:1.0 2824087514583268128:1.0 2824256572372426148:1.0 2824265298391886725:1.0 2824195996311915215:1.0 2833195510104551899:1.0 2833297693890971380:1.0 2833295087288140894:1.0 2828634743823095352:1.0 2828236856103206050:1.0|861362912965315430:1.0 861620426354557688:1.0 861952820160699846:1.0 862247359178082238:1.0 862618256609649784:1.0 862810221768640946:1.0|861577123031706908:1.0 861769181515802583:1.0 861881058815502557:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861495099862856553:1.0 861615947801238077:1.0 862127627275336382:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861556016944150930:1.0 861767744450805936:1.0 862012258868493194:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861569356112903405:1.0 861822228743483476:1.0 861950394866561113:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861414839333611708:1.0 861653716267376982:1.0 861973681977107327:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861464220456366215:1.0 861620796433787502:1.0 862029838927705188:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861395749550093080:1.0 861815418441092222:1.0 862022073955905718:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0|861388334503892902:1.0 861861330773702844:1.0 861991530376966333:1.0 862266905517130820:1.0 862501614802454984:1.0 862860227011492590:1.0;861534241222939480:1.0 861781206507797428:1.0 862006004942384875:1.0 862247359178082238:1.0 862525223368733971:1.0 862860227011492590:1.0;861519593336408472:1.0 861835802402008686:1.0 862110738087230307:1.0 862361608246017547:1.0 862610971854242761:1.0 862899130587065188:1.0;861392626822994499:1.0 861676119560622373:1.0 861934181556817485:1.0 862391853785157069:1.0 862588479281355076:1.0 862899130587065188:1.0;861578494207749664:1.0 861799326538603480:1.0 861877520699208188:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0;861434357586189860:1.0 861646188003329645:1.0 862032145175070666:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0;861502224075656678:1.0 861868527776954929:1.0 861950394866561113:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861434025215191534:1.0 861833556046309559:1.0 862034425123780065:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861458051457848172:1.0 861859206185607015:1.0 862011061326961450:1.0 862438218285668697:1.0 862517285208108214:1.0 862899130587065188:1.0;861505175510885401:1.0 861748843453638028:1.0 861878459046995469:1.0 862266905517130820:1.0 862482319750131362:1.0 862899130587065188:1.0;861457080689820141:1.0 861802504276859133:1.0 861969585531765177:1.0 862247359178082238:1.0 862525223368733971:1.0 862899130587065188:1.0;861453434146726909:1.0 861608383232980488:1.0 862105397134285933:1.0 862391853785157069:1.0 862588479281355076:1.0 862899130587065188:1.0;861314025976198703:1.0 861787646589033297:1.0 862080861605248687:1.0 862266905517130820:1.0 862581761269226550:1.0 862899130587065188:1.0;861387325012980568:1.0 861618566743921695:1.0 861908964966161203:1.0 862438218285668697:1.0 862517285208108214:1.0 862899130587065188:1.0;861466315609639829:1.0 861765326728784475:1.0 862057297027417913:1.0 862385730517928461:1.0 862443777178067152:1.0 862899130587065188:1.0;861523804510012835:1.0 861813430952576346:1.0 862035631823603456:1.0 862247359178082238:1.0 862618490933200415:1.0 862899130587065188:1.0;861370504659110060:1.0 861673413812676965:1.0 861902649919633795:1.0 862276546345913382:1.0 862706494912064744:1.0 862899130587065188:1.0;861572597071006726:1.0 861827931372330329:1.0 862012833782644024:1.0 862385730517928461:1.0 862719581526373873:1.0 862899130587065188:1.0;861524179584886857:1.0 861734506404850637:1.0 861993183080966178:1.0 862385730517928461:1.0 862529349731442931:1.0 862899130587065188:1.0;861397471342818266:1.0 861757238306588305:1.0 861971210899440333:1.0 862206651961373594:1.0 862521557091614424:1.0 862899130587065188:1.0;861538230115124050:1.0 861606082903012233:1.0 862057847764726680:1.0 862259049287616939:1.0 862514935708808651:1.0 862899130587065188:1.0;861388176907487321:1.0 861797186533421635:1.0 862024418536767103:1.0 862432239387138417:1.0 862443722727701870:1.0 862899130587065188:1.0|'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Sequence_Embedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, clicked_item_dim, pos_item_dim, unclick_item_dim, feedback_item_dim, item_dim,\n",
    "                 initializers=tf.keras.initializers.GlorotNormal(), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.clicked_item_dim = clicked_item_dim\n",
    "        self.pos_item_dim = pos_item_dim\n",
    "        self.unclick_item_dim = unclick_item_dim\n",
    "        self.feedback_item_dim = feedback_item_dim\n",
    "        self.item_dim = item_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        self.pos_w_clicked = self.add_weight(name=\"pos_w_clicked\", shape=(self.clicked_item_dim + self.pos_item_dim, self.item_dim), \n",
    "                                             initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                                             dtype=tf.float32)\n",
    "        self.pos_w_unclick = self.add_weight(name=\"pos_w_unclick\", shape=(self.unclick_item_dim + self.pos_item_dim, self.item_dim), \n",
    "                                             initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                                             dtype=tf.float32)\n",
    "        self.pos_w_feedback = self.add_weight(name=\"pos_w_feedback\", shape=(self.feedback_item_dim + self.pos_item_dim, self.item_dim), \n",
    "                                             initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                                             dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        clicked_z = tf.matmul(inputs[0], self.pos_w_clicked)\n",
    "        unclick_z = tf.matmul(inputs[1], self.pos_w_unclick)\n",
    "        feedback_z = tf.matmul(inputs[2], self.pos_w_feedback)\n",
    "        return clicked_z, unclick_z, feedback_z\n",
    "\n",
    "class Embedding_Lookup(tf.keras.layers.Layer):\n",
    "    def __init__(self, feature_size, embed_dim, initializers=tf.keras.initializers.GlorotNormal(), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_size = feature_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.initializers = initializers\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        self.embedding_w = self.add_weight(name=\"embedding_w\", shape=(self.feature_size, self.embed_dim), \n",
    "                                           initializer=self.initializers,\n",
    "                                          )\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        embedding = tf.nn.embedding_lookup_sparse(self.embedding_w, inputs, sp_weights=None, combiner='mean')\n",
    "        return embedding\n",
    "\n",
    "class Transformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hist_size, hist_embedding_dim, initializers=tf.keras.initializers.GlorotNormal(), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hist_size = hist_size\n",
    "        self.hist_embedding_dim = hist_embedding_dim\n",
    "        self.initializers = initializers\n",
    "\n",
    "    def call(self, inputs, prefix=\"\", **kwargs):\n",
    "        candidate_embedding, hist_embeddings, hisLens = inputs\n",
    "        hist_size = self.hist_size + 1\n",
    "        hist_z = [candidate_embedding]\n",
    "        for i in range(0,len(hist_embeddings)):\n",
    "            hist_z.append(hist_embeddings[i])\n",
    "        hist_z_all = tf.stack(hist_z, axis=1) #(batch, hist_size, hist_embedding_dim)\n",
    "        \n",
    "        headnum = 4\n",
    "        mutil_head_att = []\n",
    "\n",
    "        #attention\n",
    "        for i in range(0, headnum):\n",
    "            self.attQ_w = self.add_weight(name=prefix + \"attQ_w\" + str(i), shape=(self.hist_embedding_dim, int(self.hist_embedding_dim / headnum)), \n",
    "                                          initializer=self.initializers, dtype=tf.float32)\n",
    "            self.attK_w = self.add_weight(name=prefix + \"attK_w\" + str(i), shape=(self.hist_embedding_dim, int(self.hist_embedding_dim / headnum)), \n",
    "                                          initializer=self.initializers, dtype=tf.float32)\n",
    "            self.attV_w = self.add_weight(name=prefix + \"attV_w\" + str(i), shape=(self.hist_embedding_dim, int(self.hist_embedding_dim / headnum)), \n",
    "                                          initializer=self.initializers, dtype=tf.float32)\n",
    "            \n",
    "            attQ = tf.tensordot(hist_z_all, self.attQ_w, axes=1) #(batch, hist_size, hist_embedding_dim/headnum)\n",
    "            attK = tf.tensordot(hist_z_all, self.attK_w, axes=1) #(batch, hist_size, hist_embedding_dim/headnum)\n",
    "            attV = tf.tensordot(hist_z_all, self.attV_w, axes=1) #(batch, hist_size, hist_embedding_dim/headnum)\n",
    "            \n",
    "            attQK = tf.matmul(attQ, attK, transpose_b=True) #(batch, hist_size, hist_size)\n",
    "\n",
    "            #scale\n",
    "            attQK_scale = attQK / (hist_embedding_dim ** 0.5)\n",
    "            padding = tf.ones_like(attQK_scale) * (-2 ** 32 + 1) #(batch, hist_size, hist_size)\n",
    "\n",
    "            #mask\n",
    "            key_masks = tf.sequence_mask(hisLens + 1, hist_size)  # (batch, hist_size)\n",
    "            key_masks_new = tf.reshape(key_masks, [-1, 1, hist_size])\n",
    "            key_masks_tile = tf.tile(key_masks_new, [1, hist_size, 1]) #(batch, hist_size, hist_size)\n",
    "            key_masks_cast = tf.cast(key_masks_tile, dtype=tf.float32)\n",
    "            outputs_QK = tf.where(key_masks_tile, attQK_scale, padding) #(batch, hist_size, hist_size)\n",
    "\n",
    "            #norm\n",
    "            outputs_QK_norm = tf.nn.softmax(outputs_QK) #(batch, hist_size, hist_size)\n",
    "\n",
    "            #query mask\n",
    "            outputs_QK_q = tf.multiply(outputs_QK_norm, key_masks_cast) #(batch, hist_size, hist_size)\n",
    "            # weighted sum\n",
    "            outputs_QKV_head = tf.matmul(outputs_QK_q, attV) #(batch, hist_size, hist_embedding_dim/headnum)\n",
    "            mutil_head_att.append(outputs_QKV_head)\n",
    "\n",
    "        outputs_QKV = tf.concat(mutil_head_att, axis=2) # (batch, hist_size, hist_embedding_dim)\n",
    "        #FFN\n",
    "        self.FFN_w0 = self.add_weight(name=prefix + 'FFN_w0', shape=(self.hist_embedding_dim, self.hist_embedding_dim * 4), \n",
    "                                      initializer=self.initializers, dtype=tf.float32)\n",
    "        self.FFN_b0 = self.add_weight(name=prefix + 'FFN_b0', shape=(self.hist_embedding_dim * 4), \n",
    "                                      initializer=self.initializers, dtype=tf.float32)\n",
    "\n",
    "        self.FFN_w1 = self.add_weight(name=prefix + 'FFN_w1', shape=(self.hist_embedding_dim, self.hist_embedding_dim * 4), \n",
    "                                      initializer=self.initializers, dtype=tf.float32)\n",
    "        self.FFN_b1 = self.add_weight(name=prefix + 'FFN_b1', shape=(self.hist_embedding_dim * 4), \n",
    "                                      initializer=self.initializers, dtype=tf.float32)\n",
    "        \n",
    "        TH0 = tf.tensordot(outputs_QKV, self.FFN_w0, axes=1) + self.FFN_b0 #(batch, hist_size, hist_embedding_dim * 4)\n",
    "        TZ0 = tf.nn.relu(TH0)\n",
    "        TH1 = tf.tensordot(TZ0, self.FFN_w1, axes=1) + self.FFN_b1\n",
    "        # average pool\n",
    "        return tf.reduce_sum(TH1, axis=1) #(batch, hist_embedding_dim)\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, hist_size, hist_embedding_dim, initializers=tf.keras.initializers.GlorotNormal(), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hist_size = hist_size\n",
    "        self.hist_embedding_dim = hist_embedding_dim\n",
    "        self.initializers = initializers\n",
    "\n",
    "    def call(self, inputs, prefix=\"\", **kwargs):\n",
    "        candidate_embedding, hist_embeddings, hisLens = inputs\n",
    "        attention_hidden_ = 32\n",
    "        self.attW1 = self.add_weight(name=prefix + \"attention_hidden_w1\", shape=(self.hist_embedding_dim * 4, attention_hidden_), \n",
    "                                     initializer=self.initializers, dtype=tf.float32)\n",
    "        self.attB1 = self.add_weight(name=prefix + \"attention_hidden_b1\", shape=(attention_hidden_), \n",
    "                                     initializer=tf.keras.initializers.Zeros(), dtype=tf.float32)\n",
    "\n",
    "        self.attW2 = self.add_weight(name=prefix + \"attention_hidden_w2\", shape=(attention_hidden_, 1), \n",
    "                                     initializer=self.initializers, dtype=tf.float32)\n",
    "        self.attB2 = self.add_weight(name=prefix + \"attention_hidden_b2\", shape=(1), \n",
    "                                     initializer=tf.keras.initializers.Zeros(), dtype=tf.float32)\n",
    "        \n",
    "        hist_embedding_list=[]\n",
    "        for i in range(0, self.hist_size):\n",
    "            # batch, hist_embedding_dim * 4\n",
    "            z1 = tf.concat([candidate_embedding, hist_embeddings[i], candidate_embedding * hist_embeddings[i], candidate_embedding-hist_embeddings[i]], axis=1)\n",
    "            hist_embedding_list.append(z1)\n",
    "        hist_z_all = tf.stack(hist_embeddings, axis=1) #(batch, hist_size, hist_embedding_dim)\n",
    "        z2 = tf.concat(hist_embedding_list, axis=1)  #(batch, hist_size * hist_embedding_dim * 4)\n",
    "        z3 = tf.reshape(z2, [-1, self.hist_size, 4 * self.hist_embedding_dim])\n",
    "        z4 = tf.tensordot(z3, self.attW1, axes=1) + self.attB1 #(batch , hist_size, attention_hidden_)\n",
    "        z5 = tf.nn.relu(z4)\n",
    "        z6 = tf.tensordot(z5, self.attW2, axes=1) + self.attB2 #(batch, hist_size, 1)\n",
    "        att_w_all = tf.reshape(z6, [-1, self.hist_size])\n",
    "\n",
    "        #mask\n",
    "        hist_masks = tf.sequence_mask(hisLens, self.hist_size) #(batch, hist_size)\n",
    "        padding = tf.ones_like(att_w_all) * (-2**32 + 1)\n",
    "        att_w_all_rep = tf.where(hist_masks, att_w_all, padding)\n",
    "\n",
    "        #scale\n",
    "        att_w_all_scale = att_w_all_rep / (self.hist_embedding_dim ** 0.5)\n",
    "\n",
    "        #norm\n",
    "        att_w_all_norm = tf.nn.softmax(att_w_all_scale)\n",
    "\n",
    "        att_w_all_mul = tf.reshape(att_w_all_norm, [-1, 1, self.hist_size])\n",
    "        weighted_hist_all = tf.matmul(att_w_all_mul, hist_z_all) #(batch, 1, hist_embedding_dim)\n",
    "        return tf.reshape(weighted_hist_all, [-1, self.hist_embedding_dim])\n",
    "\n",
    "class DNN(tf.keras.layers.Layer):\n",
    "    \"\"\"The Multi Layer Percetron\n",
    "\n",
    "      Input shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., input_dim)``. The most common situation would be a 2D input with shape ``(batch_size, input_dim)``.\n",
    "\n",
    "      Output shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., hidden_size[-1])``. For instance, for a 2D input with shape ``(batch_size, input_dim)``, the output would have shape ``(batch_size, hidden_size[-1])``.\n",
    "\n",
    "      Arguments\n",
    "        - **hidden_units**:list of positive integer, the layer number and units in each layer.\n",
    "\n",
    "        - **activation**: Activation function to use.\n",
    "\n",
    "        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix.\n",
    "\n",
    "        - **dropout_rate**: float in [0,1). Fraction of the units to dropout.\n",
    "\n",
    "        - **use_bn**: bool. Whether use BatchNormalization before activation or not.\n",
    "\n",
    "        - **seed**: A Python integer to use as random seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False, seed=1024, **kwargs):\n",
    "        super(DNN, self).__init__(**kwargs)\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.seed = seed\n",
    "        self.l2_reg = l2_reg\n",
    "        self.use_bn = use_bn\n",
    "        self.activation_dict = {\"relu\": tf.nn.relu(),\n",
    "                                \"sigmoid\": tf.nn.sigmoid(),\n",
    "                                \"tanh\": tf.nn.tanh()}\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(DNN, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "        # if len(self.hidden_units) == 0:\n",
    "        #     raise ValueError(\"hidden_units is empty\")\n",
    "        input_size = input_shape[-1]\n",
    "        hidden_units = [int(input_size)] + list(self.hidden_units)\n",
    "        self.kernels = [self.add_weight(name='kernel' + str(i),\n",
    "                                        shape=(\n",
    "                                            hidden_units[i], hidden_units[i + 1]),\n",
    "                                            initializer=tf.keras.initializers.GlorotNormal(\n",
    "                                                seed=self.seed),\n",
    "                                        regularizer=tf.keras.regularizers.L2(self.l2_reg),\n",
    "                                        trainable=True) for i in range(len(self.hidden_units))]\n",
    "        self.bias = [self.add_weight(name='bias' + str(i),\n",
    "                                     shape=(self.hidden_units[i],),\n",
    "                                     initializer=tf.keras.initializers.Zeros(),\n",
    "                                     trainable=True) for i in range(len(self.hidden_units))]\n",
    "        if self.use_bn:\n",
    "            self.bn_layers = [tf.keras.layers.BatchNormalization() for _ in range(len(self.hidden_units))]\n",
    "\n",
    "        self.dropout_layers = [tf.keras.layers.Dropout(self.dropout_rate, seed=self.seed + i) for i in\n",
    "                               range(len(self.hidden_units))]\n",
    "\n",
    "        self.activation_layers = [self.activation_dict[self.activation] for _ in range(len(self.hidden_units))]\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "\n",
    "        deep_input = inputs\n",
    "\n",
    "        for i in range(len(self.hidden_units)):\n",
    "            fc = tf.nn.bias_add(tf.tensordot(\n",
    "                deep_input, self.kernels[i], axes=(-1, 0)), self.bias[i])\n",
    "            # fc = Dense(self.hidden_size[i], activation=None, \\\n",
    "            #           kernel_initializer=glorot_normal(seed=self.seed), \\\n",
    "            #           kernel_regularizer=l2(self.l2_reg))(deep_input)\n",
    "            if self.use_bn:\n",
    "                fc = self.bn_layers[i](fc, training=training)\n",
    "\n",
    "            fc = self.activation_layers[i](fc)\n",
    "\n",
    "            fc = self.dropout_layers[i](fc, training=training)\n",
    "            deep_input = fc\n",
    "\n",
    "        return deep_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if len(self.hidden_units) > 0:\n",
    "            shape = input_shape[:-1] + (self.hidden_units[-1],)\n",
    "        else:\n",
    "            shape = input_shape\n",
    "\n",
    "        return tuple(shape)\n",
    "\n",
    "    def get_config(self, ):\n",
    "        config = {'activation': self.activation, 'hidden_units': self.hidden_units,\n",
    "                  'l2_reg': self.l2_reg, 'use_bn': self.use_bn, 'dropout_rate': self.dropout_rate, 'seed': self.seed}\n",
    "        base_config = super(DNN, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class DFN():\n",
    "    def __init__(self, main_group_ids, candidate_group_ids, clicked_group_ids, unclick_group_ids, feedback_group_ids, \n",
    "                 pos_group_ids, batch_size=256, embed_dim=16, feature_size=1048573, hist_size=30):\n",
    "        self.main_group_ids = main_group_ids\n",
    "        self.candidate_group_ids = candidate_group_ids\n",
    "        self.clicked_group_ids = clicked_group_ids\n",
    "        self.unclick_group_ids = unclick_group_ids\n",
    "        self.feedback_group_ids = feedback_group_ids\n",
    "        self.pos_group_ids = pos_group_ids\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.feature_size = feature_size\n",
    "        self.hist_size = hist_size\n",
    "        self.group_feature = OrderedDict()\n",
    "        self.clicked_item_dim = len(clicked_group_ids) * embed_dim\n",
    "        self.unclick_item_dim = len(unclick_group_ids) * embed_dim\n",
    "        self.feedback_item_dim = len(feedback_group_ids) * embed_dim\n",
    "        self.item_dim = self.clicked_item_dim\n",
    "        self.pos_item_dim = len(pos_group_ids) * embed_dim\n",
    "        self._results = None\n",
    "        # build input\n",
    "        for group_id in main_group_ids:\n",
    "          self.group_feature[\"main_\" + str(group_id)] = tf.keras.layers.Input(shape=(self.feature_size, ), dtype=tf.int32, sparse=True, name=(\"main_\" + str(group_id)))\n",
    "        for group_id in candidate_group_ids:\n",
    "          self.group_feature[\"candidate_\" + str(group_id)] = tf.keras.layers.Input(shape=(self.feature_size, ), dtype=tf.int32, sparse=True, name=(\"candidate_\" + str(group_id)))\n",
    " \n",
    "        for i in range(0, hist_size):\n",
    "            for group_id in clicked_group_ids:\n",
    "                self.group_feature[\"clicked\" + \"_\" + str(i) + \"_\" + str(group_id)] = tf.keras.layers.Input(shape=(self.feature_size, ), dtype=tf.int32, sparse=True, name=(\"clicked\" + \"_\" + str(i) + \"_\" + str(group_id)))\n",
    "            for group_id in unclick_group_ids:\n",
    "                self.group_feature[\"unclick\" + \"_\" + str(i) + \"_\" + str(group_id)] = tf.keras.layers.Input(shape=(self.feature_size, ), dtype=tf.int32, sparse=True, name=(\"unclick\" + \"_\" + str(i) + \"_\" + str(group_id)))\n",
    "            for group_id in feedback_group_ids:\n",
    "                self.group_feature[\"feedback\" + \"_\" + str(i) + \"_\" + str(group_id)] = tf.keras.layers.Input(shape=(self.feature_size, ), dtype=tf.int32, sparse=True, name=(\"feedback\" + \"_\" + str(i)+\"_\"+str(group_id)))  \n",
    "            for group_id in pos_group_ids:\n",
    "                self.group_feature[\"clicked\" + \"_\" + \"position\" + \"_\" + str(i) + \"_\" + str(group_id)] = tf.keras.layers.Input(shape=(self.feature_size, ), dtype=tf.int32, sparse=True, name=(\"clicked\" + \"_\" + \"position\" + \"_\"+str(i) + \"_\"+str(group_id)))\n",
    "                self.group_feature[\"unclick\" + \"_\" + \"position\" + \"_\" + str(i) + \"_\" + str(group_id)] = tf.keras.layers.Input(shape=(self.feature_size, ), dtype=tf.int32, sparse=True, name=(\"unclick\" + \"_\" + \"position\" + \"_\"+str(i) + \"_\" + str(group_id)))\n",
    "                self.group_feature[\"feedback\" + \"_\" + \"position\" + \"_\" + str(i) + \"_\" + str(group_id)] = tf.keras.layers.Input(shape=(self.feature_size, ), dtype=tf.int32, sparse=True, name=(\"feedback\" + \"_\" + \"position\" + \"_\"+str(i) + \"_\" + str(group_id)))\n",
    "        self.group_feature[\"clicked_histLen\"] = tf.keras.layers.Input(shape=(1, ), dtype=tf.float32, name=(\"clicked_histLen\"))\n",
    "        self.group_feature[\"unclick_histLen\"] = tf.keras.layers.Input(shape=(1, ), dtype=tf.float32, name=(\"unclick_histLen\"))\n",
    "        self.group_feature[\"feedback_histLen\"] = tf.keras.layers.Input(shape=(1, ), dtype=tf.float32, name=(\"feedback_histLen\"))\n",
    "\n",
    "    def embedding_lookup(self, group_ids, prefix=\"\"):\n",
    "        embeddings = []\n",
    "        for group_id in group_ids:\n",
    "            embedding = self.embed_layer(self.group_feature[prefix + str(group_id)])\n",
    "            embeddings.append(embedding)\n",
    "        embedding_out = tf.concat(embeddings, axis=1)\n",
    "        return embedding_out\n",
    "\n",
    "    def __call__(self,):\n",
    "        clicked_embeddings = []\n",
    "        unclick_embeddings = []\n",
    "        feedback_embeddings = []\n",
    "        self.embed_layer = Embedding_Lookup(self.feature_size, self.embed_dim,\n",
    "                                            tf.keras.initializers.TruncatedNormal(mean=0., stddev=0.01),\n",
    "                                            name=\"embedding_w\")\n",
    "        # batch_size, len(main_group_ids) * embed_dim, 相同field之间的特征求mean\n",
    "        main_embedding = self.embedding_lookup(self.main_group_ids, prefix=\"main_\")\n",
    "\n",
    "        # batch_size, len(candidate_group_ids) * embed_dim, 相同field之间的特征求mean\n",
    "        candidate_embedding = self.embedding_lookup(self.candidate_group_ids, prefix=\"candidate_\")\n",
    "\n",
    "        seq_emb = Sequence_Embedding(self.clicked_item_dim, self.pos_item_dim, self.unclick_item_dim, self.feedback_item_dim, self.item_dim)\n",
    "        for i in range(0, self.hist_size):\n",
    "            # 一个用户不用field都有30长的序列，对于用户序列，每个序号求相同field的mean\n",
    "            # batch_size, len(clicked_group_ids) * embed_dim\n",
    "            self.embedding_lookup(self.main_group_ids, prefix=\"main_\")\n",
    "            clicked_embedding = self.embedding_lookup(self.clicked_group_ids, prefix = \"clicked\" + \"_\" + str(i) + \"_\")\n",
    "            unclick_embedding = self.embedding_lookup(self.unclick_group_ids, prefix=\"unclick\" + \"_\" + str(i) + \"_\")\n",
    "            feedback_embedding = self.embedding_lookup(self.feedback_group_ids, prefix=\"feedback\" + \"_\" + str(i) + \"_\")\n",
    "            clicked_position_embedding = self.embedding_lookup(self.pos_group_ids, prefix=\"clicked\" + \"_\" + \"position\" + \"_\" + str(i) + \"_\")\n",
    "            unclick_position_embedding = self.embedding_lookup(self.pos_group_ids, prefix=\"unclick\" + \"_\" + \"position\" + \"_\" + str(i) + \"_\")\n",
    "            feedback_position_embedding = self.embedding_lookup(self.pos_group_ids, prefix=\"feedback\" + \"_\" + \"position\" + \"_\" + str(i) + \"_\")\n",
    "            # 位置信息concat\n",
    "            clicked_pos = tf.concat([clicked_embedding, clicked_position_embedding], axis=1)\n",
    "            unclick_pos = tf.concat([unclick_embedding, unclick_position_embedding], axis=1)\n",
    "            feedback_pos = tf.concat([feedback_embedding, feedback_position_embedding], axis=1)\n",
    "            # 特征和位置embedding\n",
    "            # batch_size, len(clicked_group_ids) * embed_dim\n",
    "            clicked_z, unclick_z, feedback_z = seq_emb([clicked_pos, unclick_pos, feedback_pos])\n",
    "            clicked_embeddings.append(clicked_z)\n",
    "            unclick_embeddings.append(unclick_z)\n",
    "            feedback_embeddings.append(feedback_z)\n",
    "\n",
    "        # wide embedding\n",
    "        main_embeddings_wide = []\n",
    "        candidate_embeddings_wide = []\n",
    "        self.embed_wide = Embedding_Lookup(self.feature_size, 1, tf.keras.initializers.Zeros(), name=\"embedding_wide\")\n",
    "        for group_id in self.main_group_ids:\n",
    "            # batch_size, len(main_group_ids) * 1\n",
    "            embedding_wide = self.embed_wide(self.group_feature[\"main_\" + str(group_id)])\n",
    "            main_embeddings_wide.append(embedding_wide)\n",
    "        main_embedding_wide = tf.concat(main_embeddings_wide, axis=1)\n",
    "\n",
    "        for group_id in candidate_group_ids:\n",
    "            # batch_size, len(candidate_group_ids) * 1\n",
    "            embedding_wide = self.embed_wide(self.group_feature[\"candidate_\" + str(group_id)])\n",
    "            candidate_embeddings_wide.append(embedding_wide)\n",
    "        candidate_embedding_wide = tf.concat(candidate_embeddings_wide, axis=1)\n",
    "\n",
    "        # batch, hist_embedding_dim\n",
    "        transformer = Transformer(self.hist_size, self.item_dim)\n",
    "        output_clicked = transformer([candidate_embedding, clicked_embeddings, self.group_feature[\"clicked_histLen\"]], prefix=\"clicked\")\n",
    "        output_unclick = transformer([candidate_embedding, unclick_embeddings, self.group_feature[\"unclick_histLen\"]], prefix=\"unclick\")\n",
    "        output_feedback = transformer([candidate_embedding, feedback_embeddings, self.group_feature[\"feedback_histLen\"]], prefix=\"feedback\")\n",
    "        \n",
    "        attention = Attention(self.hist_size, self.item_dim)\n",
    "        output_unclick_clicked = attention([output_clicked, unclick_embeddings, self.group_feature[\"unclick_histLen\"]], prefix=\"unclick_clicked\")\n",
    "        output_unclick_feedback = attention([output_feedback, unclick_embeddings, self.group_feature[\"unclick_histLen\"]], prefix=\"unclick_feedback\")\n",
    "\n",
    "        input_embedding = tf.concat([main_embedding, candidate_embedding, output_clicked, output_unclick, output_feedback, output_unclick_clicked, output_unclick_feedback],axis=1)\n",
    "\n",
    "        #fm part 这个*6估计相当于multihead一样，切分成多个子空间进行两两交叉组合\n",
    "        m = len(main_group_ids) + len(candidate_group_ids) * 6\n",
    "        fm_in = tf.reshape(input_embedding, shape=[-1, m, self.embed_dim])\n",
    "        sum1 = tf.reduce_sum(fm_in, axis=1)\n",
    "        sum2 = tf.reduce_sum(fm_in * fm_in, axis=1)\n",
    "        fm = (sum1 * sum1 - sum2) * 0.5\n",
    "\n",
    "        #deep part\n",
    "        deep = DNN([32, 16])(input_embedding)\n",
    "\n",
    "        z = tf.concat([deep, fm, main_embedding_wide, candidate_embedding_wide], axis=1)\n",
    "        results = DNN([1,], activation=\"sigmoid\")\n",
    "        return tf.reshape(results, [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_group_ids=[16,10001,10002,10003,21,10006,10019,10034,20147,20148,10035,20156,\n",
    "                    61,10047,10048,10049,10050,10055,10056,60]\n",
    "candidate_group_ids=[3060,3061,3062,3063,3064]\n",
    "clicked_group_ids=[3060,3061,3062,3063,3064]\n",
    "unclick_group_ids=[3060,3061,3062,3063,3064]\n",
    "feedback_group_ids=[3060,3061,3063,3064]\n",
    "pos_group_ids=[3065]\n",
    "\n",
    "path = Path(\"/Volumes/D/guohao/resys/dfn/example\")\n",
    "dfn = DFN(main_group_ids, candidate_group_ids, clicked_group_ids, unclick_group_ids, feedback_group_ids, pos_group_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "in converted code:\n\n    <ipython-input-55-ad838a5e6e42>:85 call  *\n        attQK_scale = attQK / (hist_embedding_dim ** 0.5)\n\n    NameError: name 'hist_embedding_dim' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-37da8cc08e91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-ad838a5e6e42>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;31m# batch, hist_embedding_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[0moutput_clicked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidate_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclicked_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_feature\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clicked_histLen\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"clicked\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m         \u001b[0moutput_unclick\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidate_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munclick_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_feature\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"unclick_histLen\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unclick\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0moutput_feedback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidate_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeedback_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_feature\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"feedback_histLen\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"feedback\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    840\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[0;32m    841\u001b[0m                   \u001b[1;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 842\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m                     \u001b[1;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                     \u001b[1;31m# circular dependencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\gh software\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: in converted code:\n\n    <ipython-input-55-ad838a5e6e42>:85 call  *\n        attQK_scale = attQK / (hist_embedding_dim ** 0.5)\n\n    NameError: name 'hist_embedding_dim' is not defined\n"
     ]
    }
   ],
   "source": [
    "dfn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.keras.layers.Input(shape=(1, 30))\n",
    "b = tf.keras.layers.Input(shape=(30, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([256, 1, 80])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(a, b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
